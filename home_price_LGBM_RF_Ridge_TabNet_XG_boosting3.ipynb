{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "import lightgbm as lgb\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\nLightGBM\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "LightGBM\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/ishikawa/signate/home_price/train.csv\")\n",
    "test  = pd.read_csv(\"/home/ishikawa/signate/home_price/test.csv\")\n",
    "#train = train[train['SalePrice'] < 200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "train = train.drop(columns=['Garage Area'])\n",
    "test = test.drop(columns=[ 'Garage Area'])\n",
    "\n",
    "#train['Total_Bsmt'] = train['BsmtFin SF 1'] + train['Bsmt Unf SF']\n",
    "#test['Total_Bsmt'] = test['BsmtFin SF 1'] + test['Bsmt Unf SF']\n",
    "\n",
    "#外れ値除去で置換するときはwhereを使う\n",
    "#train['Total Bsmt SF'] = train['Total Bsmt SF'].where(train['Total Bsmt SF'] > 0, train['Total_Bsmt'])\n",
    "#test['Total Bsmt SF'] = test['Total Bsmt SF'].where(test['Total Bsmt SF'] > 0, test['Total_Bsmt'])\n",
    "\n",
    "#train = train.drop(columns=['Bsmt Full Bath','Year Remod/Add'])\n",
    "#test = test.drop(columns=[ 'Bsmt Full Bath','Year Remod/Add'])\n",
    "\n",
    "#train = train.drop(columns=['Total_Bsmt'])\n",
    "#test = test.drop(columns=[ 'Total_Bsmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = train.columns[train.dtypes == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['MS Zoning', 'Lot Shape', 'Land Contour', 'Lot Config', 'Neighborhood',\n",
    "       'Bldg Type', 'House Style', 'Roof Style', 'Exterior 1st',\n",
    "       'Exterior 2nd', 'Exter Qual', 'Foundation', 'Heating QC', 'Central Air',\n",
    "       'Electrical', 'Kitchen Qual', 'Paved Drive', 'Sale Type',\n",
    "       'Sale Condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in category:\n",
    "    train[c] = train[c].astype('category')\n",
    "    test[c]  = test[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 1223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(data = train['SalePrice'])\n",
    "del train['SalePrice']\n",
    "train_x = pd.DataFrame(data = train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective':'regression', 'metric':'rmse', 'seed':2020, 'verbose':0,\n",
    "         'lambda_l1': 0.0005110547098216984,\n",
    "         'lambda_l2': 0.006664459019340734,\n",
    "         'num_leaves': 25,\n",
    "         'feature_fraction': 0.41600000000000004,\n",
    "         'bagging_fraction': 1.0,\n",
    "         'bagging_freq': 0,\n",
    "         'min_child_samples': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "116517\tvalid's rmse: 0.139255\n",
      "[60]\ttrain's rmse: 0.116072\tvalid's rmse: 0.139393\n",
      "[61]\ttrain's rmse: 0.115484\tvalid's rmse: 0.139709\n",
      "[62]\ttrain's rmse: 0.114931\tvalid's rmse: 0.139772\n",
      "[63]\ttrain's rmse: 0.114544\tvalid's rmse: 0.139834\n",
      "[64]\ttrain's rmse: 0.114055\tvalid's rmse: 0.139743\n",
      "[65]\ttrain's rmse: 0.113567\tvalid's rmse: 0.139907\n",
      "[66]\ttrain's rmse: 0.112971\tvalid's rmse: 0.139846\n",
      "[67]\ttrain's rmse: 0.112378\tvalid's rmse: 0.140005\n",
      "[68]\ttrain's rmse: 0.111813\tvalid's rmse: 0.140018\n",
      "[69]\ttrain's rmse: 0.111278\tvalid's rmse: 0.139946\n",
      "[70]\ttrain's rmse: 0.110915\tvalid's rmse: 0.140054\n",
      "[71]\ttrain's rmse: 0.110338\tvalid's rmse: 0.139765\n",
      "[72]\ttrain's rmse: 0.109849\tvalid's rmse: 0.139907\n",
      "[73]\ttrain's rmse: 0.109375\tvalid's rmse: 0.140198\n",
      "[74]\ttrain's rmse: 0.108811\tvalid's rmse: 0.140624\n",
      "[75]\ttrain's rmse: 0.108449\tvalid's rmse: 0.140748\n",
      "[76]\ttrain's rmse: 0.108055\tvalid's rmse: 0.14076\n",
      "[77]\ttrain's rmse: 0.107722\tvalid's rmse: 0.140807\n",
      "[78]\ttrain's rmse: 0.107335\tvalid's rmse: 0.140939\n",
      "[79]\ttrain's rmse: 0.106952\tvalid's rmse: 0.140927\n",
      "[80]\ttrain's rmse: 0.106596\tvalid's rmse: 0.140889\n",
      "[81]\ttrain's rmse: 0.106251\tvalid's rmse: 0.140678\n",
      "[82]\ttrain's rmse: 0.105866\tvalid's rmse: 0.140797\n",
      "[83]\ttrain's rmse: 0.105349\tvalid's rmse: 0.140932\n",
      "[84]\ttrain's rmse: 0.104892\tvalid's rmse: 0.140949\n",
      "[85]\ttrain's rmse: 0.104484\tvalid's rmse: 0.14107\n",
      "[86]\ttrain's rmse: 0.104025\tvalid's rmse: 0.141208\n",
      "[87]\ttrain's rmse: 0.103626\tvalid's rmse: 0.141304\n",
      "[88]\ttrain's rmse: 0.103282\tvalid's rmse: 0.141118\n",
      "[89]\ttrain's rmse: 0.102935\tvalid's rmse: 0.141234\n",
      "[90]\ttrain's rmse: 0.10244\tvalid's rmse: 0.141179\n",
      "[91]\ttrain's rmse: 0.10205\tvalid's rmse: 0.141156\n",
      "[92]\ttrain's rmse: 0.10168\tvalid's rmse: 0.141155\n",
      "[93]\ttrain's rmse: 0.101249\tvalid's rmse: 0.141162\n",
      "[94]\ttrain's rmse: 0.100851\tvalid's rmse: 0.141313\n",
      "[95]\ttrain's rmse: 0.100534\tvalid's rmse: 0.141241\n",
      "[96]\ttrain's rmse: 0.10007\tvalid's rmse: 0.14141\n",
      "[97]\ttrain's rmse: 0.099702\tvalid's rmse: 0.141483\n",
      "[98]\ttrain's rmse: 0.099283\tvalid's rmse: 0.141525\n",
      "[99]\ttrain's rmse: 0.0989214\tvalid's rmse: 0.141323\n",
      "[100]\ttrain's rmse: 0.0986156\tvalid's rmse: 0.141363\n",
      "[101]\ttrain's rmse: 0.0982485\tvalid's rmse: 0.141638\n",
      "[102]\ttrain's rmse: 0.0978528\tvalid's rmse: 0.141639\n",
      "[103]\ttrain's rmse: 0.0975326\tvalid's rmse: 0.14147\n",
      "[104]\ttrain's rmse: 0.0969922\tvalid's rmse: 0.141602\n",
      "[105]\ttrain's rmse: 0.0965578\tvalid's rmse: 0.141527\n",
      "[106]\ttrain's rmse: 0.0962759\tvalid's rmse: 0.141617\n",
      "[107]\ttrain's rmse: 0.0958824\tvalid's rmse: 0.141546\n",
      "[108]\ttrain's rmse: 0.0954733\tvalid's rmse: 0.141607\n",
      "[109]\ttrain's rmse: 0.0949879\tvalid's rmse: 0.141845\n",
      "[110]\ttrain's rmse: 0.0946256\tvalid's rmse: 0.141758\n",
      "[111]\ttrain's rmse: 0.0943518\tvalid's rmse: 0.141892\n",
      "[112]\ttrain's rmse: 0.0939031\tvalid's rmse: 0.142113\n",
      "[113]\ttrain's rmse: 0.0935227\tvalid's rmse: 0.142029\n",
      "[114]\ttrain's rmse: 0.0931376\tvalid's rmse: 0.142108\n",
      "[115]\ttrain's rmse: 0.0928058\tvalid's rmse: 0.142214\n",
      "[116]\ttrain's rmse: 0.0924543\tvalid's rmse: 0.142232\n",
      "[117]\ttrain's rmse: 0.092062\tvalid's rmse: 0.142075\n",
      "[118]\ttrain's rmse: 0.0917684\tvalid's rmse: 0.14224\n",
      "[119]\ttrain's rmse: 0.0915211\tvalid's rmse: 0.142331\n",
      "[120]\ttrain's rmse: 0.0912837\tvalid's rmse: 0.142274\n",
      "[121]\ttrain's rmse: 0.0911035\tvalid's rmse: 0.142182\n",
      "[122]\ttrain's rmse: 0.0907782\tvalid's rmse: 0.142299\n",
      "[123]\ttrain's rmse: 0.0904242\tvalid's rmse: 0.14227\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttrain's rmse: 0.1428\tvalid's rmse: 0.138607\n",
      "20478.48832154729\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[1]\ttrain's rmse: 0.18261\tvalid's rmse: 0.193148\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's rmse: 0.17828\tvalid's rmse: 0.189737\n",
      "[3]\ttrain's rmse: 0.174489\tvalid's rmse: 0.186386\n",
      "[4]\ttrain's rmse: 0.171251\tvalid's rmse: 0.18367\n",
      "[5]\ttrain's rmse: 0.168121\tvalid's rmse: 0.182747\n",
      "[6]\ttrain's rmse: 0.165463\tvalid's rmse: 0.180799\n",
      "[7]\ttrain's rmse: 0.16308\tvalid's rmse: 0.178862\n",
      "[8]\ttrain's rmse: 0.160735\tvalid's rmse: 0.177353\n",
      "[9]\ttrain's rmse: 0.158706\tvalid's rmse: 0.17595\n",
      "[10]\ttrain's rmse: 0.156829\tvalid's rmse: 0.1749\n",
      "[11]\ttrain's rmse: 0.15496\tvalid's rmse: 0.173625\n",
      "[12]\ttrain's rmse: 0.153357\tvalid's rmse: 0.172544\n",
      "[13]\ttrain's rmse: 0.15185\tvalid's rmse: 0.171788\n",
      "[14]\ttrain's rmse: 0.150488\tvalid's rmse: 0.171057\n",
      "[15]\ttrain's rmse: 0.149044\tvalid's rmse: 0.17055\n",
      "[16]\ttrain's rmse: 0.147753\tvalid's rmse: 0.170155\n",
      "[17]\ttrain's rmse: 0.14667\tvalid's rmse: 0.169497\n",
      "[18]\ttrain's rmse: 0.145632\tvalid's rmse: 0.169424\n",
      "[19]\ttrain's rmse: 0.144646\tvalid's rmse: 0.169199\n",
      "[20]\ttrain's rmse: 0.143643\tvalid's rmse: 0.168988\n",
      "[21]\ttrain's rmse: 0.142555\tvalid's rmse: 0.168847\n",
      "[22]\ttrain's rmse: 0.141519\tvalid's rmse: 0.168991\n",
      "[23]\ttrain's rmse: 0.140652\tvalid's rmse: 0.168935\n",
      "[24]\ttrain's rmse: 0.139705\tvalid's rmse: 0.169355\n",
      "[25]\ttrain's rmse: 0.138727\tvalid's rmse: 0.169398\n",
      "[26]\ttrain's rmse: 0.13812\tvalid's rmse: 0.169365\n",
      "[27]\ttrain's rmse: 0.137362\tvalid's rmse: 0.16923\n",
      "[28]\ttrain's rmse: 0.136387\tvalid's rmse: 0.168996\n",
      "[29]\ttrain's rmse: 0.135575\tvalid's rmse: 0.169216\n",
      "[30]\ttrain's rmse: 0.134738\tvalid's rmse: 0.169075\n",
      "[31]\ttrain's rmse: 0.134055\tvalid's rmse: 0.169443\n",
      "[32]\ttrain's rmse: 0.133298\tvalid's rmse: 0.169435\n",
      "[33]\ttrain's rmse: 0.132581\tvalid's rmse: 0.169374\n",
      "[34]\ttrain's rmse: 0.131757\tvalid's rmse: 0.169405\n",
      "[35]\ttrain's rmse: 0.13109\tvalid's rmse: 0.169402\n",
      "[36]\ttrain's rmse: 0.130389\tvalid's rmse: 0.169527\n",
      "[37]\ttrain's rmse: 0.129631\tvalid's rmse: 0.169405\n",
      "[38]\ttrain's rmse: 0.129019\tvalid's rmse: 0.169228\n",
      "[39]\ttrain's rmse: 0.128296\tvalid's rmse: 0.169196\n",
      "[40]\ttrain's rmse: 0.127601\tvalid's rmse: 0.169208\n",
      "[41]\ttrain's rmse: 0.126788\tvalid's rmse: 0.169139\n",
      "[42]\ttrain's rmse: 0.126263\tvalid's rmse: 0.169191\n",
      "[43]\ttrain's rmse: 0.12559\tvalid's rmse: 0.169057\n",
      "[44]\ttrain's rmse: 0.124917\tvalid's rmse: 0.168819\n",
      "[45]\ttrain's rmse: 0.124238\tvalid's rmse: 0.168904\n",
      "[46]\ttrain's rmse: 0.123683\tvalid's rmse: 0.169203\n",
      "[47]\ttrain's rmse: 0.123154\tvalid's rmse: 0.16914\n",
      "[48]\ttrain's rmse: 0.122311\tvalid's rmse: 0.169264\n",
      "[49]\ttrain's rmse: 0.121776\tvalid's rmse: 0.169396\n",
      "[50]\ttrain's rmse: 0.121231\tvalid's rmse: 0.169243\n",
      "[51]\ttrain's rmse: 0.12056\tvalid's rmse: 0.169571\n",
      "[52]\ttrain's rmse: 0.119923\tvalid's rmse: 0.169529\n",
      "[53]\ttrain's rmse: 0.119466\tvalid's rmse: 0.169599\n",
      "[54]\ttrain's rmse: 0.118899\tvalid's rmse: 0.169639\n",
      "[55]\ttrain's rmse: 0.118291\tvalid's rmse: 0.169802\n",
      "[56]\ttrain's rmse: 0.117792\tvalid's rmse: 0.169796\n",
      "[57]\ttrain's rmse: 0.117286\tvalid's rmse: 0.169542\n",
      "[58]\ttrain's rmse: 0.116689\tvalid's rmse: 0.169579\n",
      "[59]\ttrain's rmse: 0.116139\tvalid's rmse: 0.169493\n",
      "[60]\ttrain's rmse: 0.115678\tvalid's rmse: 0.169349\n",
      "[61]\ttrain's rmse: 0.115088\tvalid's rmse: 0.169659\n",
      "[62]\ttrain's rmse: 0.114688\tvalid's rmse: 0.169596\n",
      "[63]\ttrain's rmse: 0.114102\tvalid's rmse: 0.169761\n",
      "[64]\ttrain's rmse: 0.113506\tvalid's rmse: 0.169885\n",
      "[65]\ttrain's rmse: 0.113038\tvalid's rmse: 0.169726\n",
      "[66]\ttrain's rmse: 0.112584\tvalid's rmse: 0.169776\n",
      "[67]\ttrain's rmse: 0.112203\tvalid's rmse: 0.169863\n",
      "[68]\ttrain's rmse: 0.111679\tvalid's rmse: 0.170023\n",
      "[69]\ttrain's rmse: 0.11128\tvalid's rmse: 0.169981\n",
      "[70]\ttrain's rmse: 0.110709\tvalid's rmse: 0.170214\n",
      "[71]\ttrain's rmse: 0.110172\tvalid's rmse: 0.170302\n",
      "[72]\ttrain's rmse: 0.10971\tvalid's rmse: 0.17026\n",
      "[73]\ttrain's rmse: 0.109315\tvalid's rmse: 0.170268\n",
      "[74]\ttrain's rmse: 0.108917\tvalid's rmse: 0.170105\n",
      "[75]\ttrain's rmse: 0.10845\tvalid's rmse: 0.170326\n",
      "[76]\ttrain's rmse: 0.108001\tvalid's rmse: 0.170031\n",
      "[77]\ttrain's rmse: 0.107684\tvalid's rmse: 0.170032\n",
      "[78]\ttrain's rmse: 0.107267\tvalid's rmse: 0.170154\n",
      "[79]\ttrain's rmse: 0.106936\tvalid's rmse: 0.170393\n",
      "[80]\ttrain's rmse: 0.106538\tvalid's rmse: 0.170456\n",
      "[81]\ttrain's rmse: 0.106107\tvalid's rmse: 0.170562\n",
      "[82]\ttrain's rmse: 0.105782\tvalid's rmse: 0.170653\n",
      "[83]\ttrain's rmse: 0.105456\tvalid's rmse: 0.170767\n",
      "[84]\ttrain's rmse: 0.104927\tvalid's rmse: 0.170994\n",
      "[85]\ttrain's rmse: 0.104556\tvalid's rmse: 0.171131\n",
      "[86]\ttrain's rmse: 0.104072\tvalid's rmse: 0.170939\n",
      "[87]\ttrain's rmse: 0.103618\tvalid's rmse: 0.17093\n",
      "[88]\ttrain's rmse: 0.10313\tvalid's rmse: 0.170824\n",
      "[89]\ttrain's rmse: 0.102769\tvalid's rmse: 0.170881\n",
      "[90]\ttrain's rmse: 0.102417\tvalid's rmse: 0.170959\n",
      "[91]\ttrain's rmse: 0.102031\tvalid's rmse: 0.170869\n",
      "[92]\ttrain's rmse: 0.101582\tvalid's rmse: 0.170517\n",
      "[93]\ttrain's rmse: 0.101318\tvalid's rmse: 0.170578\n",
      "[94]\ttrain's rmse: 0.100874\tvalid's rmse: 0.170859\n",
      "[95]\ttrain's rmse: 0.100533\tvalid's rmse: 0.170946\n",
      "[96]\ttrain's rmse: 0.100052\tvalid's rmse: 0.17098\n",
      "[97]\ttrain's rmse: 0.099657\tvalid's rmse: 0.17094\n",
      "[98]\ttrain's rmse: 0.0992518\tvalid's rmse: 0.170704\n",
      "[99]\ttrain's rmse: 0.0988666\tvalid's rmse: 0.170881\n",
      "[100]\ttrain's rmse: 0.0984478\tvalid's rmse: 0.170957\n",
      "[101]\ttrain's rmse: 0.0980412\tvalid's rmse: 0.170984\n",
      "[102]\ttrain's rmse: 0.0975707\tvalid's rmse: 0.170913\n",
      "[103]\ttrain's rmse: 0.0971814\tvalid's rmse: 0.170806\n",
      "[104]\ttrain's rmse: 0.0968526\tvalid's rmse: 0.170722\n",
      "[105]\ttrain's rmse: 0.0964314\tvalid's rmse: 0.170721\n",
      "[106]\ttrain's rmse: 0.0961852\tvalid's rmse: 0.170606\n",
      "[107]\ttrain's rmse: 0.095785\tvalid's rmse: 0.170769\n",
      "[108]\ttrain's rmse: 0.0953993\tvalid's rmse: 0.170757\n",
      "[109]\ttrain's rmse: 0.0951823\tvalid's rmse: 0.170734\n",
      "[110]\ttrain's rmse: 0.0948877\tvalid's rmse: 0.170893\n",
      "[111]\ttrain's rmse: 0.094625\tvalid's rmse: 0.171069\n",
      "[112]\ttrain's rmse: 0.0942536\tvalid's rmse: 0.171232\n",
      "[113]\ttrain's rmse: 0.0939527\tvalid's rmse: 0.171182\n",
      "[114]\ttrain's rmse: 0.0936011\tvalid's rmse: 0.171393\n",
      "[115]\ttrain's rmse: 0.0932739\tvalid's rmse: 0.171398\n",
      "[116]\ttrain's rmse: 0.0930653\tvalid's rmse: 0.17128\n",
      "[117]\ttrain's rmse: 0.0926919\tvalid's rmse: 0.17138\n",
      "[118]\ttrain's rmse: 0.092343\tvalid's rmse: 0.171382\n",
      "[119]\ttrain's rmse: 0.0919952\tvalid's rmse: 0.171436\n",
      "[120]\ttrain's rmse: 0.0916124\tvalid's rmse: 0.171526\n",
      "[121]\ttrain's rmse: 0.0912987\tvalid's rmse: 0.171545\n",
      "[122]\ttrain's rmse: 0.0910175\tvalid's rmse: 0.171494\n",
      "[123]\ttrain's rmse: 0.0907561\tvalid's rmse: 0.171482\n",
      "[124]\ttrain's rmse: 0.0903368\tvalid's rmse: 0.171535\n",
      "[125]\ttrain's rmse: 0.0900255\tvalid's rmse: 0.171517\n",
      "[126]\ttrain's rmse: 0.089691\tvalid's rmse: 0.17148\n",
      "[127]\ttrain's rmse: 0.0894169\tvalid's rmse: 0.171506\n",
      "[128]\ttrain's rmse: 0.089048\tvalid's rmse: 0.171461\n",
      "[129]\ttrain's rmse: 0.0887017\tvalid's rmse: 0.171555\n",
      "[130]\ttrain's rmse: 0.0884179\tvalid's rmse: 0.171341\n",
      "[131]\ttrain's rmse: 0.0880647\tvalid's rmse: 0.171346\n",
      "[132]\ttrain's rmse: 0.0876306\tvalid's rmse: 0.171295\n",
      "[133]\ttrain's rmse: 0.087365\tvalid's rmse: 0.171394\n",
      "[134]\ttrain's rmse: 0.0869775\tvalid's rmse: 0.171483\n",
      "[135]\ttrain's rmse: 0.0866529\tvalid's rmse: 0.171174\n",
      "[136]\ttrain's rmse: 0.086363\tvalid's rmse: 0.171463\n",
      "[137]\ttrain's rmse: 0.08608\tvalid's rmse: 0.171374\n",
      "[138]\ttrain's rmse: 0.0858495\tvalid's rmse: 0.171338\n",
      "[139]\ttrain's rmse: 0.0855684\tvalid's rmse: 0.171219\n",
      "[140]\ttrain's rmse: 0.0852543\tvalid's rmse: 0.171227\n",
      "[141]\ttrain's rmse: 0.0849257\tvalid's rmse: 0.171369\n",
      "[142]\ttrain's rmse: 0.0845813\tvalid's rmse: 0.171611\n",
      "[143]\ttrain's rmse: 0.084169\tvalid's rmse: 0.171437\n",
      "[144]\ttrain's rmse: 0.0838962\tvalid's rmse: 0.171352\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttrain's rmse: 0.124917\tvalid's rmse: 0.168819\n",
      "27224.47189811639\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[1]\ttrain's rmse: 0.183304\tvalid's rmse: 0.184486\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's rmse: 0.179246\tvalid's rmse: 0.181641\n",
      "[3]\ttrain's rmse: 0.175188\tvalid's rmse: 0.178389\n",
      "[4]\ttrain's rmse: 0.171895\tvalid's rmse: 0.175708\n",
      "[5]\ttrain's rmse: 0.168971\tvalid's rmse: 0.172945\n",
      "[6]\ttrain's rmse: 0.166327\tvalid's rmse: 0.171182\n",
      "[7]\ttrain's rmse: 0.164087\tvalid's rmse: 0.169836\n",
      "[8]\ttrain's rmse: 0.161974\tvalid's rmse: 0.168647\n",
      "[9]\ttrain's rmse: 0.160083\tvalid's rmse: 0.167391\n",
      "[10]\ttrain's rmse: 0.157989\tvalid's rmse: 0.166276\n",
      "[11]\ttrain's rmse: 0.156202\tvalid's rmse: 0.164864\n",
      "[12]\ttrain's rmse: 0.154712\tvalid's rmse: 0.163929\n",
      "[13]\ttrain's rmse: 0.152977\tvalid's rmse: 0.163354\n",
      "[14]\ttrain's rmse: 0.151534\tvalid's rmse: 0.162666\n",
      "[15]\ttrain's rmse: 0.15009\tvalid's rmse: 0.162333\n",
      "[16]\ttrain's rmse: 0.148725\tvalid's rmse: 0.16165\n",
      "[17]\ttrain's rmse: 0.147627\tvalid's rmse: 0.160955\n",
      "[18]\ttrain's rmse: 0.146354\tvalid's rmse: 0.160284\n",
      "[19]\ttrain's rmse: 0.145236\tvalid's rmse: 0.160127\n",
      "[20]\ttrain's rmse: 0.144174\tvalid's rmse: 0.159885\n",
      "[21]\ttrain's rmse: 0.142963\tvalid's rmse: 0.159435\n",
      "[22]\ttrain's rmse: 0.141746\tvalid's rmse: 0.159409\n",
      "[23]\ttrain's rmse: 0.140864\tvalid's rmse: 0.159121\n",
      "[24]\ttrain's rmse: 0.139999\tvalid's rmse: 0.158949\n",
      "[25]\ttrain's rmse: 0.139112\tvalid's rmse: 0.158685\n",
      "[26]\ttrain's rmse: 0.138183\tvalid's rmse: 0.157816\n",
      "[27]\ttrain's rmse: 0.137376\tvalid's rmse: 0.157537\n",
      "[28]\ttrain's rmse: 0.136565\tvalid's rmse: 0.157485\n",
      "[29]\ttrain's rmse: 0.135729\tvalid's rmse: 0.157329\n",
      "[30]\ttrain's rmse: 0.134979\tvalid's rmse: 0.157842\n",
      "[31]\ttrain's rmse: 0.134176\tvalid's rmse: 0.157592\n",
      "[32]\ttrain's rmse: 0.133263\tvalid's rmse: 0.157448\n",
      "[33]\ttrain's rmse: 0.132451\tvalid's rmse: 0.157641\n",
      "[34]\ttrain's rmse: 0.131779\tvalid's rmse: 0.157603\n",
      "[35]\ttrain's rmse: 0.130969\tvalid's rmse: 0.158121\n",
      "[36]\ttrain's rmse: 0.130402\tvalid's rmse: 0.158113\n",
      "[37]\ttrain's rmse: 0.129617\tvalid's rmse: 0.158244\n",
      "[38]\ttrain's rmse: 0.128831\tvalid's rmse: 0.158137\n",
      "[39]\ttrain's rmse: 0.128152\tvalid's rmse: 0.158099\n",
      "[40]\ttrain's rmse: 0.127533\tvalid's rmse: 0.158292\n",
      "[41]\ttrain's rmse: 0.12702\tvalid's rmse: 0.158318\n",
      "[42]\ttrain's rmse: 0.126402\tvalid's rmse: 0.15798\n",
      "[43]\ttrain's rmse: 0.125738\tvalid's rmse: 0.157871\n",
      "[44]\ttrain's rmse: 0.125008\tvalid's rmse: 0.157912\n",
      "[45]\ttrain's rmse: 0.124386\tvalid's rmse: 0.15774\n",
      "[46]\ttrain's rmse: 0.123834\tvalid's rmse: 0.157766\n",
      "[47]\ttrain's rmse: 0.123197\tvalid's rmse: 0.157521\n",
      "[48]\ttrain's rmse: 0.12258\tvalid's rmse: 0.157606\n",
      "[49]\ttrain's rmse: 0.121937\tvalid's rmse: 0.157522\n",
      "[50]\ttrain's rmse: 0.121358\tvalid's rmse: 0.157657\n",
      "[51]\ttrain's rmse: 0.120719\tvalid's rmse: 0.157287\n",
      "[52]\ttrain's rmse: 0.120082\tvalid's rmse: 0.15733\n",
      "[53]\ttrain's rmse: 0.119409\tvalid's rmse: 0.157772\n",
      "[54]\ttrain's rmse: 0.118961\tvalid's rmse: 0.157672\n",
      "[55]\ttrain's rmse: 0.118471\tvalid's rmse: 0.157515\n",
      "[56]\ttrain's rmse: 0.117986\tvalid's rmse: 0.157583\n",
      "[57]\ttrain's rmse: 0.117405\tvalid's rmse: 0.157507\n",
      "[58]\ttrain's rmse: 0.117003\tvalid's rmse: 0.157259\n",
      "[59]\ttrain's rmse: 0.116456\tvalid's rmse: 0.157128\n",
      "[60]\ttrain's rmse: 0.115898\tvalid's rmse: 0.157197\n",
      "[61]\ttrain's rmse: 0.115165\tvalid's rmse: 0.157472\n",
      "[62]\ttrain's rmse: 0.114661\tvalid's rmse: 0.157573\n",
      "[63]\ttrain's rmse: 0.114239\tvalid's rmse: 0.157413\n",
      "[64]\ttrain's rmse: 0.113768\tvalid's rmse: 0.157742\n",
      "[65]\ttrain's rmse: 0.113353\tvalid's rmse: 0.15757\n",
      "[66]\ttrain's rmse: 0.112857\tvalid's rmse: 0.15776\n",
      "[67]\ttrain's rmse: 0.112289\tvalid's rmse: 0.157761\n",
      "[68]\ttrain's rmse: 0.111792\tvalid's rmse: 0.15781\n",
      "[69]\ttrain's rmse: 0.111297\tvalid's rmse: 0.157791\n",
      "[70]\ttrain's rmse: 0.110892\tvalid's rmse: 0.158128\n",
      "[71]\ttrain's rmse: 0.110378\tvalid's rmse: 0.158317\n",
      "[72]\ttrain's rmse: 0.110006\tvalid's rmse: 0.158458\n",
      "[73]\ttrain's rmse: 0.109549\tvalid's rmse: 0.158523\n",
      "[74]\ttrain's rmse: 0.109026\tvalid's rmse: 0.158663\n",
      "[75]\ttrain's rmse: 0.108663\tvalid's rmse: 0.158725\n",
      "[76]\ttrain's rmse: 0.108155\tvalid's rmse: 0.158504\n",
      "[77]\ttrain's rmse: 0.107782\tvalid's rmse: 0.15872\n",
      "[78]\ttrain's rmse: 0.10733\tvalid's rmse: 0.158874\n",
      "[79]\ttrain's rmse: 0.10684\tvalid's rmse: 0.158768\n",
      "[80]\ttrain's rmse: 0.106626\tvalid's rmse: 0.158673\n",
      "[81]\ttrain's rmse: 0.106331\tvalid's rmse: 0.158702\n",
      "[82]\ttrain's rmse: 0.105899\tvalid's rmse: 0.158638\n",
      "[83]\ttrain's rmse: 0.10541\tvalid's rmse: 0.158551\n",
      "[84]\ttrain's rmse: 0.104937\tvalid's rmse: 0.158829\n",
      "[85]\ttrain's rmse: 0.104463\tvalid's rmse: 0.158802\n",
      "[86]\ttrain's rmse: 0.104031\tvalid's rmse: 0.158794\n",
      "[87]\ttrain's rmse: 0.10356\tvalid's rmse: 0.15892\n",
      "[88]\ttrain's rmse: 0.103265\tvalid's rmse: 0.159282\n",
      "[89]\ttrain's rmse: 0.10285\tvalid's rmse: 0.159026\n",
      "[90]\ttrain's rmse: 0.10248\tvalid's rmse: 0.159024\n",
      "[91]\ttrain's rmse: 0.102092\tvalid's rmse: 0.159114\n",
      "[92]\ttrain's rmse: 0.101752\tvalid's rmse: 0.158985\n",
      "[93]\ttrain's rmse: 0.101483\tvalid's rmse: 0.159082\n",
      "[94]\ttrain's rmse: 0.101125\tvalid's rmse: 0.159044\n",
      "[95]\ttrain's rmse: 0.100779\tvalid's rmse: 0.159135\n",
      "[96]\ttrain's rmse: 0.100318\tvalid's rmse: 0.159312\n",
      "[97]\ttrain's rmse: 0.0999791\tvalid's rmse: 0.159324\n",
      "[98]\ttrain's rmse: 0.099658\tvalid's rmse: 0.15949\n",
      "[99]\ttrain's rmse: 0.0993899\tvalid's rmse: 0.159399\n",
      "[100]\ttrain's rmse: 0.0990129\tvalid's rmse: 0.159447\n",
      "[101]\ttrain's rmse: 0.0985624\tvalid's rmse: 0.159625\n",
      "[102]\ttrain's rmse: 0.0982349\tvalid's rmse: 0.159899\n",
      "[103]\ttrain's rmse: 0.0978788\tvalid's rmse: 0.159881\n",
      "[104]\ttrain's rmse: 0.0975929\tvalid's rmse: 0.159914\n",
      "[105]\ttrain's rmse: 0.0972521\tvalid's rmse: 0.159822\n",
      "[106]\ttrain's rmse: 0.0969401\tvalid's rmse: 0.159736\n",
      "[107]\ttrain's rmse: 0.0966552\tvalid's rmse: 0.159722\n",
      "[108]\ttrain's rmse: 0.0962797\tvalid's rmse: 0.159724\n",
      "[109]\ttrain's rmse: 0.0959168\tvalid's rmse: 0.159726\n",
      "[110]\ttrain's rmse: 0.0955472\tvalid's rmse: 0.159666\n",
      "[111]\ttrain's rmse: 0.0951445\tvalid's rmse: 0.15975\n",
      "[112]\ttrain's rmse: 0.0947912\tvalid's rmse: 0.159781\n",
      "[113]\ttrain's rmse: 0.0944657\tvalid's rmse: 0.159967\n",
      "[114]\ttrain's rmse: 0.0940337\tvalid's rmse: 0.159942\n",
      "[115]\ttrain's rmse: 0.0936182\tvalid's rmse: 0.15978\n",
      "[116]\ttrain's rmse: 0.0932527\tvalid's rmse: 0.159717\n",
      "[117]\ttrain's rmse: 0.0929968\tvalid's rmse: 0.159648\n",
      "[118]\ttrain's rmse: 0.0925972\tvalid's rmse: 0.159772\n",
      "[119]\ttrain's rmse: 0.0922729\tvalid's rmse: 0.159704\n",
      "[120]\ttrain's rmse: 0.0920129\tvalid's rmse: 0.159804\n",
      "[121]\ttrain's rmse: 0.091741\tvalid's rmse: 0.159752\n",
      "[122]\ttrain's rmse: 0.0913444\tvalid's rmse: 0.16001\n",
      "[123]\ttrain's rmse: 0.0910412\tvalid's rmse: 0.16001\n",
      "[124]\ttrain's rmse: 0.090634\tvalid's rmse: 0.160022\n",
      "[125]\ttrain's rmse: 0.0903141\tvalid's rmse: 0.159793\n",
      "[126]\ttrain's rmse: 0.0900292\tvalid's rmse: 0.159822\n",
      "[127]\ttrain's rmse: 0.0897295\tvalid's rmse: 0.159786\n",
      "[128]\ttrain's rmse: 0.0893904\tvalid's rmse: 0.159695\n",
      "[129]\ttrain's rmse: 0.0890735\tvalid's rmse: 0.159834\n",
      "[130]\ttrain's rmse: 0.0887806\tvalid's rmse: 0.159776\n",
      "[131]\ttrain's rmse: 0.0883415\tvalid's rmse: 0.159934\n",
      "[132]\ttrain's rmse: 0.0880987\tvalid's rmse: 0.159943\n",
      "[133]\ttrain's rmse: 0.0878474\tvalid's rmse: 0.159921\n",
      "[134]\ttrain's rmse: 0.0875817\tvalid's rmse: 0.160071\n",
      "[135]\ttrain's rmse: 0.0872656\tvalid's rmse: 0.160029\n",
      "[136]\ttrain's rmse: 0.0869384\tvalid's rmse: 0.160193\n",
      "[137]\ttrain's rmse: 0.0866741\tvalid's rmse: 0.160392\n",
      "[138]\ttrain's rmse: 0.0864142\tvalid's rmse: 0.160389\n",
      "[139]\ttrain's rmse: 0.0861155\tvalid's rmse: 0.160456\n",
      "[140]\ttrain's rmse: 0.0858461\tvalid's rmse: 0.160395\n",
      "[141]\ttrain's rmse: 0.0855715\tvalid's rmse: 0.160544\n",
      "[142]\ttrain's rmse: 0.0852238\tvalid's rmse: 0.16053\n",
      "[143]\ttrain's rmse: 0.0848632\tvalid's rmse: 0.160627\n",
      "[144]\ttrain's rmse: 0.0846309\tvalid's rmse: 0.160709\n",
      "[145]\ttrain's rmse: 0.0843174\tvalid's rmse: 0.160687\n",
      "[146]\ttrain's rmse: 0.0840999\tvalid's rmse: 0.160548\n",
      "[147]\ttrain's rmse: 0.0838607\tvalid's rmse: 0.160676\n",
      "[148]\ttrain's rmse: 0.0836411\tvalid's rmse: 0.160607\n",
      "[149]\ttrain's rmse: 0.0833828\tvalid's rmse: 0.160503\n",
      "[150]\ttrain's rmse: 0.0829807\tvalid's rmse: 0.160536\n",
      "[151]\ttrain's rmse: 0.082712\tvalid's rmse: 0.160503\n",
      "[152]\ttrain's rmse: 0.0824635\tvalid's rmse: 0.160481\n",
      "[153]\ttrain's rmse: 0.0821829\tvalid's rmse: 0.160314\n",
      "[154]\ttrain's rmse: 0.0818775\tvalid's rmse: 0.160222\n",
      "[155]\ttrain's rmse: 0.0816848\tvalid's rmse: 0.16034\n",
      "[156]\ttrain's rmse: 0.0813275\tvalid's rmse: 0.160427\n",
      "[157]\ttrain's rmse: 0.0809696\tvalid's rmse: 0.160342\n",
      "[158]\ttrain's rmse: 0.0807013\tvalid's rmse: 0.160319\n",
      "[159]\ttrain's rmse: 0.0803958\tvalid's rmse: 0.160412\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttrain's rmse: 0.116456\tvalid's rmse: 0.157128\n",
      "25082.330235508136\n"
     ]
    }
   ],
   "source": [
    "lgb_preds = []\n",
    "rmses = []\n",
    "lgb_va_idxes = []\n",
    "lgb_va_preds = []\n",
    "\n",
    "for i, (tr_idx,va_idx) in enumerate(kfold.split(train_x)):\n",
    "    \n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(tr_x, tr_y, categorical_feature = category)\n",
    "    lgb_eval  = lgb.Dataset(va_x, va_y, categorical_feature = category)\n",
    "    \n",
    "    lgb_model = lgb.train(params, lgb_train, num_boost_round = 2000, early_stopping_rounds = 100,\n",
    "                         valid_names=['train', 'valid'], valid_sets=[lgb_train, lgb_eval])\n",
    "    \n",
    "    va_pred = lgb_model.predict(va_x,num_iteration = lgb_model.best_iteration)\n",
    "    \n",
    "    tmp_rmse = np.sqrt(mean_squared_error(np.exp(va_pred), np.exp(va_y))) #SalePriceのlog変換対応\n",
    "    \n",
    "    rmses.append(tmp_rmse)\n",
    "    print(tmp_rmse)\n",
    "    \n",
    "    lgb_va_preds.append(va_pred)\n",
    "    lgb_va_idxes.append(va_idx)\n",
    "    \n",
    "    pred = lgb_model.predict(test, num_iteration = lgb_model.best_iteration)\n",
    "    lgb_preds.append(pred) #SalePriceのlog変換対応\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96866936, 11.98104582, 11.91597281, 12.07938988, 11.97350284,\n",
       "        11.97051834, 11.77611726, 11.83089333, 11.97827451, 12.07228611,\n",
       "        11.97650079, 11.84520937, 11.89173036, 11.89039169, 11.9242377 ,\n",
       "        12.03287653, 12.14989634, 11.76306805, 11.806612  , 11.9216752 ,\n",
       "        12.04036951, 11.74676573, 11.89563368, 11.97121416, 12.01582859,\n",
       "        11.79023581, 11.93148022, 11.78695091, 11.73185462, 11.87498761,\n",
       "        11.94421773, 11.98454684, 11.943669  , 11.81632738, 11.7373451 ,\n",
       "        11.92246373, 12.03684407, 11.87699335, 11.97415147, 11.97237128,\n",
       "        11.97801743, 11.97790092, 11.88757188, 11.8663472 , 11.96740133,\n",
       "        11.9654736 , 11.97408216, 11.82902896, 11.78688594, 11.9232482 ,\n",
       "        12.05631331, 11.77895883, 11.92596153, 11.89011739, 11.91208579,\n",
       "        11.87917825, 11.85043404, 11.61737306, 11.8065988 , 11.9933722 ,\n",
       "        11.86728664, 11.97205951, 11.93158912, 12.06548531, 11.94404612,\n",
       "        11.81813802, 11.69326402, 11.98612792, 12.03460743, 11.88466918,\n",
       "        11.75114976, 11.88059388, 11.9000674 , 11.86607333, 11.98193351,\n",
       "        11.80655689, 11.96906497, 11.67943699, 11.86256334, 11.89608716,\n",
       "        11.72234213, 12.09463568, 11.79348141, 12.11062722, 11.73545561,\n",
       "        11.94876105, 11.84394913, 11.78201665, 11.82571843, 11.88428659,\n",
       "        11.88670833, 11.75230605, 11.96889052, 12.05411964, 11.74359701,\n",
       "        11.94291372, 11.66284143, 11.87078922, 11.71603809, 11.90899459,\n",
       "        11.9752074 , 12.06421156, 11.91889931, 11.97425751, 11.95307916,\n",
       "        11.72635987, 11.81968443, 12.10472307, 11.92954916, 11.97329935,\n",
       "        11.97116307, 11.78907944, 11.93915957, 11.78613234, 11.90032123,\n",
       "        11.93957384, 12.01281554, 11.96884042, 11.84699928, 11.91399452,\n",
       "        11.85755134, 12.07383895, 11.96821887, 11.78369809, 11.93029254,\n",
       "        11.96804156, 12.11494191, 11.64452482, 11.85984123, 11.73465276,\n",
       "        11.7683047 , 11.78512068, 11.9814497 , 11.91661978, 11.90678794,\n",
       "        11.96992812, 11.99826927, 11.87615956, 11.93431741, 11.81154939,\n",
       "        11.95757614, 11.92151428, 12.10444246, 11.94749472, 11.99699441,\n",
       "        11.76977385, 11.97962599, 11.88865699, 11.7593795 , 11.87431067,\n",
       "        11.6569665 , 11.8550933 , 11.9644519 , 11.80590419, 11.74185328,\n",
       "        11.88100219, 11.97352829, 11.97085709, 12.01767776, 11.96878208,\n",
       "        11.93408162, 11.95569442, 11.66473823, 12.01793014, 11.96224249,\n",
       "        11.8062653 , 11.89364636, 12.04574767, 11.86795718, 11.84840688]),\n",
       " array([11.97347988, 12.04026274, 11.84971094, 11.8613301 , 11.68501616,\n",
       "        11.87166838, 11.83816336, 11.75678527, 11.97060923, 11.91445072,\n",
       "        11.97511611, 11.80862079, 11.77561672, 12.2170836 , 11.97356424,\n",
       "        11.90291651, 11.77592048, 12.08877708, 12.08256995, 12.1026053 ,\n",
       "        11.87599843, 11.94386056, 11.85023574, 11.97105386, 11.9711548 ,\n",
       "        12.01389475, 11.68633443, 11.98294129, 11.98153335, 11.82995677,\n",
       "        11.80649098, 12.0012537 , 11.86706396, 11.84540516, 11.94247351,\n",
       "        11.96501133, 11.87736999, 11.81763258, 11.7940484 , 11.99490443,\n",
       "        11.88349089, 11.97004369, 12.01834552, 11.97027463, 11.98538915,\n",
       "        12.05972276, 11.92074474, 11.83649089, 12.02794229, 12.05790016,\n",
       "        11.73155763, 11.70740298, 11.74511812, 11.81197256, 11.71512066,\n",
       "        11.86875082, 11.96845243, 11.85791156, 12.05796411, 11.8067084 ,\n",
       "        11.86835041, 11.97216763, 11.88966864, 11.87637572, 11.88067825,\n",
       "        11.86344205, 11.97313857, 11.90167299, 12.12390835, 11.80024142,\n",
       "        11.78683619, 11.85005923, 11.72706255, 11.96578888, 11.84619962,\n",
       "        11.9163424 , 11.87338239, 12.10384273, 11.79419012, 11.97167355,\n",
       "        11.96392164, 11.77991213, 11.87244781, 11.83531382, 12.04376505,\n",
       "        11.85183722, 11.85706527, 12.03983452, 11.77160192, 11.82265787,\n",
       "        11.86855075, 11.8017165 , 11.85854245, 11.94072259, 12.00642763,\n",
       "        11.72733912, 11.85623576, 11.8760968 , 12.08607762, 11.91543468,\n",
       "        11.87272692, 11.91685632, 11.8576742 , 11.83646107, 11.87044287,\n",
       "        11.82671216, 12.05891308, 11.97213538, 11.82830927, 11.94933599,\n",
       "        11.95743114, 11.85935967, 11.84839075, 11.84430231, 11.97187671,\n",
       "        11.9709039 , 12.00164207, 11.9355599 , 11.74976561, 11.86919496,\n",
       "        11.94872164, 12.01969827, 11.93317083, 11.87278321, 11.9422958 ,\n",
       "        11.69727987, 12.00827415, 11.92279961, 11.85601616, 11.78401644,\n",
       "        12.00084719, 11.97879086, 11.74940312, 12.05886861, 11.85627034,\n",
       "        11.70795292, 11.97146789, 11.97155696, 11.97874447, 11.9148042 ,\n",
       "        11.95071775, 11.97146789, 11.96980964, 12.02048761, 11.80176953,\n",
       "        11.97362644, 11.83807884, 11.86981112, 11.71754298, 11.66522951,\n",
       "        11.81421596, 11.97433139, 12.07081728, 11.97130476, 11.84218025,\n",
       "        11.87860474, 11.8032945 , 12.00010914, 11.96486472, 11.92568445,\n",
       "        11.97383542, 11.97219331, 11.96326287, 11.88337147, 11.89084974,\n",
       "        11.85406603, 12.0573907 , 11.97098759, 11.89218545, 11.72197308,\n",
       "        11.84272801, 11.93410464, 11.90901949, 11.87830877, 11.95898092,\n",
       "        11.90302987, 12.02582667, 11.81976353, 11.95916225, 11.97015877,\n",
       "        11.87795932, 11.94671697, 11.7782196 , 12.01965538, 11.88015944,\n",
       "        11.95520667, 11.97130672, 11.87429765, 11.97117508, 11.80392592,\n",
       "        11.69372214, 11.77372969, 11.97268408, 11.99972365, 12.02031155,\n",
       "        11.97407897, 11.97050126, 11.96892136, 11.91139909, 11.97105386,\n",
       "        11.72709113, 11.70657507, 11.94877237, 11.8534458 , 11.9709039 ,\n",
       "        11.81680664, 11.92726062, 11.99885534, 11.74275645, 12.12034948,\n",
       "        11.82515476, 11.90817333, 11.91201397, 11.97565744, 11.88247995,\n",
       "        11.86921356, 12.14151974, 11.86814501, 11.97219331, 11.69554766,\n",
       "        11.86857168, 11.92302371, 11.78498053, 12.10090758, 11.92149047,\n",
       "        12.00641146, 11.74636919, 11.80523238, 11.70977245, 11.96916044,\n",
       "        11.93847951, 11.97130069, 11.87517994, 11.75767105, 11.68784994,\n",
       "        11.97154963, 11.84403361, 11.94685842, 11.79088857, 11.7727251 ,\n",
       "        12.01599714, 11.86962985, 11.97130069, 11.89172423, 11.97154963,\n",
       "        11.91774016, 11.82031014, 12.10499233, 11.88537457, 11.96988981,\n",
       "        11.77988921, 12.00628107, 11.8564516 , 11.97356008, 11.85797883,\n",
       "        11.97953408, 11.861557  , 11.6533324 , 11.97154963, 11.80195574,\n",
       "        11.93495261, 11.84932315, 11.97150968, 11.89109906, 11.98272728,\n",
       "        11.86101769, 11.96902865, 11.90504764, 11.92823739, 11.77942399,\n",
       "        11.95560353, 11.77476271, 11.78258795, 11.97027463, 12.00949331,\n",
       "        11.85401837, 11.77726714, 12.14651204, 12.01813356, 11.8303843 ,\n",
       "        12.00817594, 11.95200041, 11.97842768, 11.97154963, 11.97027463,\n",
       "        11.99902756, 11.9823173 , 11.77000285, 11.91944844, 11.86926724,\n",
       "        12.02961853, 11.8513051 , 11.9638699 , 11.83394063, 11.86813516,\n",
       "        11.96598707, 11.77456931, 11.97167355, 11.85009935, 12.00219715]),\n",
       " array([11.96991665, 11.96580858, 11.93348032, 11.94289805, 11.96507436,\n",
       "        11.82742002, 11.99389267, 11.83732681, 11.84939474, 11.82760742,\n",
       "        12.03691501, 11.92555942, 11.82757798, 11.92611463, 11.8450572 ,\n",
       "        11.91360466, 11.94300781, 11.94292491, 12.04269297, 11.83901297,\n",
       "        11.95762193, 11.96867168, 11.96580858, 11.92987672, 11.82846899,\n",
       "        11.90173924, 11.88339324, 11.98748209, 11.86724645, 11.85388066,\n",
       "        11.89052707, 11.94507559, 11.94968263, 12.00799237, 11.77334483,\n",
       "        11.96249673, 11.77461285, 11.98656327, 11.92343891, 11.89909083,\n",
       "        11.96580858, 11.96401852, 11.94164683, 11.84126189, 12.02898116,\n",
       "        11.91161148, 11.83937128, 11.75597712, 11.83828929, 11.82682464,\n",
       "        11.91498198, 11.85663836, 11.96580858, 11.92588052, 11.80988491,\n",
       "        11.94292491, 11.85206325, 11.96401852, 11.96580858, 11.77712708,\n",
       "        11.91154737, 11.98391005, 11.83968777, 11.88081634, 11.95653566,\n",
       "        11.98019727, 11.95903765, 11.81273291, 12.12175618, 11.91731904,\n",
       "        11.91362519, 11.80440901, 11.82420831, 11.95903765, 11.91572001,\n",
       "        11.83299122, 12.03175314, 11.80025152, 11.75360673, 12.00057045,\n",
       "        11.85256453, 11.75146343, 11.96262632, 11.8767756 , 11.84521394,\n",
       "        11.95977175, 11.96587256, 11.86016638, 11.94373435, 11.97766627,\n",
       "        11.96580858, 11.85557879, 11.93174551, 11.75166904, 11.77890015,\n",
       "        11.93915592, 11.87142986, 12.01052014, 11.72937807, 11.92554975,\n",
       "        11.80101053, 11.95293469, 11.94004897, 11.93431788, 11.97012045,\n",
       "        11.86428157, 11.78875992, 11.93071596, 11.78218652, 11.98721901,\n",
       "        11.90489637, 11.93092888, 11.87073471, 11.86676147, 11.85597404,\n",
       "        11.84890155, 11.79264282, 11.79813978, 11.92347174, 11.96401852,\n",
       "        11.84595252, 11.8206553 , 11.74843406, 11.96580858, 11.98711242,\n",
       "        11.92773497, 11.93291986, 11.83080765, 11.93252877, 11.82057854,\n",
       "        11.88687118, 11.86162523, 11.96428679, 11.93577192, 11.87076962,\n",
       "        11.80770588, 12.01658814, 12.04692647, 12.00606312, 11.80134499,\n",
       "        11.7586093 , 11.85301661, 11.89131239, 11.94798024, 11.90187577,\n",
       "        12.00294242, 11.98770338, 11.86969762, 11.94827475, 11.92863095,\n",
       "        11.85940065, 11.88875877, 11.79509022, 11.89168523, 11.82511092,\n",
       "        11.85279022, 11.82075944, 11.89568957, 11.86608466, 12.01165384,\n",
       "        11.78238641, 11.97277975, 11.91847356, 11.84810348, 11.83049571,\n",
       "        11.9483547 , 11.92854427, 12.04392106, 11.9486501 , 12.00000986,\n",
       "        11.96748433, 11.77453269, 11.78066633, 11.78123568, 11.96522294,\n",
       "        11.82200454, 12.00962702, 11.96991665, 11.90686963, 11.88961072,\n",
       "        11.7945994 , 11.96850837, 11.82177152, 11.89624161, 11.85382388,\n",
       "        11.98073783, 11.96217344, 11.91972735, 11.96688161, 11.96343837,\n",
       "        11.91395089, 11.93585837, 11.85355637, 11.78823638, 11.98808717,\n",
       "        11.98156546, 11.96191108, 11.8931713 , 11.74103807, 11.96516302,\n",
       "        11.92500261, 11.84633009, 11.98401739, 11.80364658, 11.96580858,\n",
       "        11.86836276, 11.83504086, 11.83757872, 11.96428679, 11.85892852,\n",
       "        11.96580858, 11.80800655, 11.81928727, 11.83780717, 11.77903668,\n",
       "        11.78204448, 12.02673949, 11.90680924, 11.7660206 , 11.96580858,\n",
       "        11.81452966, 11.92500897, 11.96617763, 11.92471813, 11.85429211,\n",
       "        11.9813766 , 11.75416759, 11.84244697, 11.84215035, 11.96065164,\n",
       "        11.96894437, 12.04587545, 11.82744668, 11.83570336, 11.98029206,\n",
       "        11.80178008, 11.96522294, 11.69562193, 11.85521467, 11.95944537,\n",
       "        11.7596919 , 12.09838461, 11.94372816, 11.89349334, 11.71340039,\n",
       "        11.86887252, 11.96401852, 11.88281832, 11.83219789, 11.87148848,\n",
       "        11.96401852, 11.95903765, 11.88545047, 11.95082075, 11.83609799,\n",
       "        11.87127942, 11.8428512 , 11.96580858, 11.83166113, 11.90811029,\n",
       "        11.95336686, 11.9346224 , 11.95643786, 11.965045  , 11.92706187,\n",
       "        11.81757855, 11.70327485, 11.84519856, 11.91727533, 11.69307585,\n",
       "        11.93084559, 11.96401852, 11.96580858, 11.89188681, 11.93429456,\n",
       "        11.97267449, 11.93205095, 11.95820203, 11.79768531, 11.97725204,\n",
       "        11.82249553, 11.84991817, 11.77001594, 11.84588933, 11.772329  ,\n",
       "        11.85417511, 12.06718481, 11.82535034, 11.96580858, 11.8463001 ,\n",
       "        11.93723161, 11.89159321, 11.70564238, 12.10114151, 11.96580858,\n",
       "        11.93088569, 11.96401852, 11.84690347, 11.81259612, 11.96522294]),\n",
       " array([11.87648092, 11.86129481, 11.84783792, 11.72624178, 11.85836742,\n",
       "        11.98078153, 11.82502214, 11.83795964, 11.96992044, 12.05019167,\n",
       "        12.09144513, 11.93399058, 11.87490658, 11.91582405, 11.96861179,\n",
       "        11.88514525, 11.9632908 , 11.97116457, 11.77894411, 11.75512181,\n",
       "        11.74053598, 11.70865373, 11.81111605, 11.86743342, 11.88085425,\n",
       "        11.97413346, 11.74141831, 11.82723067, 11.8638818 , 11.86053711,\n",
       "        11.99023762, 11.77263049, 11.89252639, 11.85210394, 11.89076901,\n",
       "        11.84762318, 12.01171139, 11.6808189 , 11.8461505 , 11.83157969,\n",
       "        12.03915829, 11.78077179, 11.79898988, 12.01487909, 11.9948435 ,\n",
       "        11.83387171, 11.90219955, 11.95668343, 11.87637707, 11.70989684,\n",
       "        11.81906039, 12.02880617, 11.92379023, 11.76095721, 11.68916391,\n",
       "        12.00262068, 11.96840162, 11.96908396, 11.98093934, 11.96750856,\n",
       "        11.97658462, 11.82074444, 11.90647188, 11.85907452, 11.97766877,\n",
       "        11.8385428 , 11.84646792, 11.98652892, 11.99373413, 11.9571412 ,\n",
       "        11.9385603 , 11.70768381, 12.08445665, 11.75644624, 11.9334031 ,\n",
       "        11.8026769 , 11.95222645, 11.89290375, 12.06854823, 11.82714735,\n",
       "        11.86538184, 12.09264792, 11.99372657, 11.76828672, 11.74468853,\n",
       "        11.97034679, 11.95720055, 11.69982831, 11.90856754, 11.844526  ,\n",
       "        12.05281769, 11.96992044, 11.78178119, 11.87065787, 11.77765576,\n",
       "        11.98484664, 11.91850339, 11.96817035, 11.69652815, 11.89951229,\n",
       "        11.86767219, 11.84416075, 11.99099225, 11.90521406, 11.93610841,\n",
       "        11.9928095 , 11.86797343, 11.9701108 , 11.98521604, 12.11133149,\n",
       "        12.0397028 , 11.96833289, 11.88810511, 11.90023637, 11.84725191,\n",
       "        11.96506731, 11.97087657, 12.01603571, 11.96769892, 11.74944041,\n",
       "        11.96992044, 11.96580186, 11.73687276, 11.75051983, 11.81226971,\n",
       "        11.96984523, 12.04308604, 11.86844843, 11.80700659, 12.00527604,\n",
       "        11.82004903, 11.98878009, 11.76436179, 11.68245354, 12.01098604,\n",
       "        11.87239374, 12.06800724, 11.82617006, 11.86145412, 11.98129769,\n",
       "        11.88084128, 11.80958302, 11.83537761, 11.93651957, 11.96123383,\n",
       "        11.89759664, 11.83876951, 11.81807926, 11.96214042, 11.98139879,\n",
       "        11.9272825 , 11.90390932, 11.95930242, 11.97088252, 11.87687856,\n",
       "        11.90382364, 11.89329238, 11.8565718 , 11.97128673, 11.75068686,\n",
       "        11.82940904, 11.94845695, 11.97492232, 11.88849381, 11.81765257,\n",
       "        11.7846022 , 11.80339115, 11.79453527, 11.71274682, 11.98468487,\n",
       "        11.78212589, 11.64256397, 11.83211381, 11.73452097, 11.79326023,\n",
       "        11.92556717, 11.95208555, 12.04072241, 11.83598006, 11.96222324,\n",
       "        11.96553527, 11.83751097, 11.89616908, 12.09086602, 11.84268075,\n",
       "        11.95161951, 11.90928311, 11.97223554, 11.70436343, 11.90709007,\n",
       "        11.83491989, 11.95836176, 12.10661825, 12.04684513, 11.7644228 ,\n",
       "        11.97804833, 11.91558775, 11.94285119, 11.93145529, 11.96139677,\n",
       "        11.96508033, 11.87133073, 11.98521604, 11.95425092, 11.8267603 ,\n",
       "        11.97170516, 11.87303121, 11.95808904, 11.81216866, 11.85216957,\n",
       "        11.97247355, 11.92278047, 11.77163838, 11.83414149, 11.97339189,\n",
       "        12.08257666, 12.02119321, 11.96490618, 11.9314759 , 11.83200633,\n",
       "        11.7535277 , 11.8618066 , 11.86715664, 11.90076544, 11.83282796,\n",
       "        11.75979115, 11.90314464, 12.01058522, 11.98217059, 12.02460191,\n",
       "        11.72870256, 11.9688936 , 12.01615129, 11.87773196, 12.03477125,\n",
       "        11.73736278, 11.84937655, 11.84622795, 11.99417853, 11.75041093,\n",
       "        11.96959473, 11.93351611, 11.96900368, 11.97165332, 11.87680417,\n",
       "        11.7902705 , 11.97141451, 11.96553527, 11.97241267, 11.79139919,\n",
       "        11.95360897, 11.88274459, 11.72022884, 11.8938937 , 11.73701486,\n",
       "        11.95196812, 11.95225698, 11.93413079, 11.79061245, 11.81219944,\n",
       "        11.87571787, 11.97222144, 11.87423149, 11.76091072, 12.03100246,\n",
       "        11.89593619, 11.8364485 , 11.96982801, 11.9701108 , 11.78768402,\n",
       "        11.79696912, 11.86172822, 11.84107249, 11.97652862, 11.78081513,\n",
       "        11.85050267, 11.84819756, 11.92156455, 11.73953698, 11.96886538,\n",
       "        11.97233912, 11.96164521, 11.92239267, 11.80365883, 11.7557717 ,\n",
       "        11.97141562, 11.96769892, 11.96992044, 11.96241139, 11.85838817,\n",
       "        11.95925707, 11.78170993, 12.03041381, 11.84843558, 11.97197026,\n",
       "        11.86836757, 11.87652235, 11.81969148, 12.09186401, 11.80309707]),\n",
       " array([11.98456752, 11.97190386, 11.80938235, 11.92040362, 12.05272193,\n",
       "        12.03645423, 11.90185637, 11.96683369, 11.81991686, 12.05721252,\n",
       "        12.12195587, 11.78768906, 11.77974425, 11.81852202, 11.70159266,\n",
       "        11.97817243, 11.89173387, 12.08385939, 11.93888525, 11.90570415,\n",
       "        11.97138403, 11.91126737, 11.74622467, 11.96536561, 11.86050493,\n",
       "        12.02895419, 12.16814665, 11.71632072, 11.85107264, 11.64819122,\n",
       "        11.74517204, 11.96980279, 11.97174233, 11.87162247, 11.81339212,\n",
       "        11.97173769, 11.78799228, 11.8787424 , 12.02694272, 11.97099774,\n",
       "        11.89046059, 11.93666188, 11.93810803, 12.08571525, 11.76301052,\n",
       "        12.08615007, 11.95346507, 11.78595969, 11.77997551, 11.99540705,\n",
       "        12.10925302, 11.88149982, 11.97101282, 11.97638256, 11.70476267,\n",
       "        11.96829786, 11.69701515, 11.95485764, 11.94935892, 11.97138403,\n",
       "        11.8037981 , 12.10105883, 11.79519998, 11.91233591, 11.97099774,\n",
       "        11.86230489, 11.75201436, 11.9551173 , 12.12836865, 12.11643827,\n",
       "        11.76633806, 11.79248376, 11.86387632, 11.81633986, 11.9093273 ,\n",
       "        11.87748489, 11.66562989, 11.97638256, 11.89892952, 11.86221929,\n",
       "        11.95793719, 11.94543978, 11.96648412, 11.97004543, 11.83406887,\n",
       "        11.79224487, 12.11625088, 11.83915606, 11.88624175, 11.96039096,\n",
       "        11.98995186, 11.94263539, 11.78647739, 12.04810583, 12.011792  ,\n",
       "        12.01971401, 11.96084827, 12.22798096, 11.85850797, 11.75083919,\n",
       "        11.91322858, 12.07791074, 11.96585263, 11.82895456, 12.04792755,\n",
       "        11.81385294, 11.80830969, 11.76605415, 11.9623224 , 11.76729114,\n",
       "        11.77490826, 11.97665095, 11.76538485, 11.97689037, 11.97004543,\n",
       "        11.95894877, 11.96881586, 11.97187536, 12.09530593, 11.86138988,\n",
       "        11.9673625 , 11.82158357, 11.93478576, 11.95433549, 11.91713209,\n",
       "        11.96648412, 11.93937033, 11.85063702, 11.97638256, 11.97099774,\n",
       "        11.64602626, 11.97213907, 12.19259431, 11.86833691, 11.93555768,\n",
       "        11.74316016, 11.75044222, 11.89091255, 11.83797826, 11.8406446 ,\n",
       "        11.95573252, 12.05610943, 11.95233002, 11.73402838, 11.89884466,\n",
       "        11.93881106, 11.97945944, 12.09590048, 12.0093846 , 11.97167549,\n",
       "        11.84630107, 11.74487105, 12.03980745, 11.93814893, 11.93983885,\n",
       "        11.96077419, 11.97004543, 11.97401082, 11.80083235, 11.98094892,\n",
       "        11.98199167, 11.77208718, 11.76317408, 11.84567502, 11.86774725,\n",
       "        11.90462934, 11.92573149, 11.8291998 , 11.92283736, 11.74241177,\n",
       "        11.73357176, 11.97099774, 11.99262058, 11.90866781, 11.88298915,\n",
       "        12.07579487, 11.90381559, 12.00760664, 11.96558537, 11.9788805 ,\n",
       "        11.97269463, 11.76910264, 12.0399562 , 11.76434665, 12.07082128,\n",
       "        11.97638256, 11.95425363, 11.87697043, 11.9028317 , 11.82401442,\n",
       "        11.73789358, 11.97269463, 12.0883739 , 11.97310293, 11.96458697,\n",
       "        11.97622383, 11.97676885, 12.04184661, 11.85276378, 11.6755906 ,\n",
       "        12.17237311, 11.89707106, 11.85248081, 12.01213427, 11.89056225,\n",
       "        11.8619935 , 11.85928433, 11.91700467, 11.85536548, 11.82055707,\n",
       "        11.84468389, 11.9600251 , 11.96494902, 11.98209208, 11.99027545,\n",
       "        11.88629644, 12.08696821, 11.91038521, 12.01995335, 11.99988508,\n",
       "        11.66275844, 11.76560629, 11.94237248, 11.92283179, 11.79067517,\n",
       "        12.01050286, 11.97099774, 12.20946904, 11.84039447, 11.8694003 ,\n",
       "        11.84453364, 11.93989263, 11.79423753, 11.73038365, 12.0032973 ,\n",
       "        11.97097019, 11.89816289, 12.06339472, 11.97004543, 11.81424494,\n",
       "        11.90124223, 12.07836943, 11.85614903, 11.97004543, 11.92459112,\n",
       "        11.59828218, 12.0740109 , 11.79039425, 11.90332401, 12.08242044,\n",
       "        11.97138403, 11.97004543, 12.12256393, 12.0316981 , 11.97043172,\n",
       "        11.97004543, 11.67034799, 12.0239371 , 11.97099774, 11.79443443,\n",
       "        11.95248717, 11.90703832, 11.72984942, 11.93769867, 11.97530797,\n",
       "        12.01683835, 11.97138403, 12.01985591, 11.73410435, 11.84071806,\n",
       "        11.97890192, 11.95233765, 11.78682097, 12.01615276, 11.92364857,\n",
       "        12.0490169 , 11.82774409, 11.73430604, 11.80170866, 11.79985012,\n",
       "        11.93472885, 11.79384708, 11.99036344, 11.73864359, 11.92710384,\n",
       "        12.07550547, 11.8273222 , 11.79713476, 11.84812554, 11.86845575,\n",
       "        11.96648412, 12.00561955, 11.98722229, 11.84457485, 11.95848279,\n",
       "        11.86649853, 11.88971884, 11.86907127, 11.75454478, 11.91094283])]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "lgb_va_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_idxes_lgb = np.concatenate(lgb_va_idxes)\n",
    "va_pred_lgb = np.concatenate(lgb_va_preds,axis = 0)\n",
    "#order = np.argsort(va_idxes_lgb)\n",
    "va_pred_lgb = pd.DataFrame(data = va_pred_lgb)\n",
    "order       = pd.DataFrame(data = va_idxes_lgb)\n",
    "pred_train_lgb = pd.concat([order,va_pred_lgb],axis = 1)\n",
    "pred_train_lgb.columns = [\"id\",\"Pred_LGBM\"]\n",
    "\n",
    "preds_test_lgb = np.mean(lgb_preds, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  Pred_LGBM\n",
       "0   1  11.920807\n",
       "1   7  11.825732\n",
       "2  10  12.134273\n",
       "3  16  11.992688\n",
       "4  23  12.134359"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_LGBM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11.920807</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>11.825732</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>12.134273</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>11.992688</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>12.134359</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "pred_train_lgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\nRandomForest\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "RandomForest\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/ishikawa/signate/home_price/train.csv\")\n",
    "test  = pd.read_csv(\"/home/ishikawa/signate/home_price/test.csv\")\n",
    "#train = train[train['SalePrice'] < 200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = train.columns[train.dtypes == \"object\"]\n",
    "all_x = pd.concat([train, test])\n",
    "all_x_one = pd.get_dummies(all_x, columns = category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one = all_x_one.iloc[:train.shape[0], :].reset_index(drop = True)\n",
    "test_one  = all_x_one.iloc[train.shape[0]:, :].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(data = train_one)\n",
    "test = pd.DataFrame(data = test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_one\n",
    "del test_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "\n",
    "train = train.drop(columns=['Garage Area'])\n",
    "test = test.drop(columns=[ 'Garage Area'])\n",
    "\n",
    "#train['Total_Bsmt'] = train['BsmtFin SF 1'] + train['Bsmt Unf SF']\n",
    "#test['Total_Bsmt'] = test['BsmtFin SF 1'] + test['Bsmt Unf SF']\n",
    "\n",
    "#train['Total Bsmt SF'] = train['Total Bsmt SF'].where(train['Total Bsmt SF'] > 0, train['Total_Bsmt'])\n",
    "#test['Total Bsmt SF'] = test['Total Bsmt SF'].where(test['Total Bsmt SF'] > 0, test['Total_Bsmt'])\n",
    "\n",
    "#train = train.drop(columns=['Bsmt Full Bath','Year Remod/Add'])\n",
    "#test = test.drop(columns=[ 'Bsmt Full Bath','Year Remod/Add'])\n",
    "\n",
    "#train = train.drop(columns=['Total_Bsmt'])\n",
    "#test = test.drop(columns=[ 'Total_Bsmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train['SalePrice'])\n",
    "del train['SalePrice']\n",
    "train_x = pd.DataFrame(data = train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RFR(n_estimators = 1500, random_state = 1223, max_depth = 70 )\n",
    "kFold = KFold(n_splits = 10, shuffle = True, random_state = 2020)\n",
    "preds_RF = []\n",
    "rmses = []\n",
    "va_idxes_RF = []\n",
    "va_preds_RF = []\n",
    "\n",
    "for tr_idx, va_idx in kFold.split(train_x):\n",
    "    \n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    RF_model.fit(tr_x, tr_y)\n",
    "    \n",
    "    va_pred = RF_model.predict(va_x)\n",
    "    \n",
    "    tmp_rmse = np.sqrt(mean_squared_error(np.exp(va_pred), np.exp(va_y)))\n",
    "    rmses.append(tmp_rmse)\n",
    "    \n",
    "    va_preds_RF.append(va_pred)\n",
    "    va_idxes_RF.append(va_idx)\n",
    "    \n",
    "    pred = RF_model.predict(test)\n",
    "    preds_RF.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id    Pred_RF\n",
       "0  10  12.095845\n",
       "1  13  11.786684\n",
       "2  14  12.085356\n",
       "3  31  11.915181\n",
       "4  37  11.965273"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_RF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>12.095845</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13</td>\n      <td>11.786684</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14</td>\n      <td>12.085356</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31</td>\n      <td>11.915181</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37</td>\n      <td>11.965273</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "va_idxes_RF = np.concatenate(va_idxes_RF)\n",
    "va_pred_RF = np.concatenate(va_preds_RF,axis = 0)\n",
    "#order = np.argsort(va_idxes_RF)\n",
    "va_pred_RF = pd.DataFrame(data = va_pred_RF)\n",
    "order       = pd.DataFrame(data = va_idxes_RF)\n",
    "pred_train_RF = pd.concat([order,va_pred_RF],axis = 1)\n",
    "\n",
    "pred_train_RF.columns = [\"id\",\"Pred_RF\"]\n",
    "\n",
    "preds_test_RF = np.mean(preds_RF, axis = 0)\n",
    "\n",
    "pred_train_RF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\nRidge Liner regression\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Ridge Liner regression\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/ishikawa/signate/home_price/train.csv\")\n",
    "test  = pd.read_csv(\"/home/ishikawa/signate/home_price/test.csv\")\n",
    "\n",
    "#train = train[train['SalePrice'] < 200000]\n",
    "\n",
    "category = train.columns[train.dtypes == \"object\"]\n",
    "all_x = pd.concat([train, test])\n",
    "all_x_one = pd.get_dummies(all_x, columns = category)\n",
    "\n",
    "train_one = all_x_one.iloc[:train.shape[0], :].reset_index(drop = True)\n",
    "test_one  = all_x_one.iloc[train.shape[0]:, :].reset_index(drop = True)\n",
    "\n",
    "train = pd.DataFrame(data = train_one)\n",
    "test = pd.DataFrame(data = test_one)\n",
    "\n",
    "del train_one\n",
    "del test_one\n",
    "\n",
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "\n",
    "train = train.drop(columns=['Garage Area'])\n",
    "test = test.drop(columns=[ 'Garage Area'])\n",
    "'''\n",
    "train['Total_Bsmt'] = train['BsmtFin SF 1'] + train['Bsmt Unf SF']\n",
    "test['Total_Bsmt'] = test['BsmtFin SF 1'] + test['Bsmt Unf SF']\n",
    "\n",
    "train['Total Bsmt SF'] = train['Total Bsmt SF'].where(train['Total Bsmt SF'] > 0, train['Total_Bsmt'])\n",
    "test['Total Bsmt SF'] = test['Total Bsmt SF'].where(test['Total Bsmt SF'] > 0, test['Total_Bsmt'])\n",
    "\n",
    "train = train.drop(columns=['Bsmt Full Bath','Year Remod/Add'])\n",
    "test = test.drop(columns=[ 'Bsmt Full Bath','Year Remod/Add'])\n",
    "\n",
    "train = train.drop(columns=['Total_Bsmt'])\n",
    "test = test.drop(columns=[ 'Total_Bsmt'])\n",
    "'''\n",
    "train_y = pd.DataFrame(train['SalePrice'])\n",
    "del train['SalePrice']\n",
    "train_x = pd.DataFrame(data = train)\n",
    "del train\n",
    "\n",
    "del test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kFold = KFold(n_splits = 10, shuffle = True, random_state = 2020)\n",
    "preds_LR = []\n",
    "rmses = []\n",
    "va_idxes_LR = []\n",
    "va_preds_LR = []\n",
    "\n",
    "ridge_model = linear_model.Ridge(alpha = 0.01)\n",
    "\n",
    "for tr_idx, va_idx in kFold.split(train_x):\n",
    "    \n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    ridge_model.fit(tr_x,tr_y)\n",
    "    \n",
    "    va_pred = ridge_model.predict(va_x)\n",
    "    va_preds_LR.append(va_pred)\n",
    "    \n",
    "    va_idxes_LR.append(va_idx)\n",
    "    \n",
    "    tmp_rmse = np.sqrt(mean_squared_error(np.exp(va_pred), np.exp(va_y)))\n",
    "    rmses.append(tmp_rmse)\n",
    "    \n",
    "    pred = ridge_model.predict(test)\n",
    "    preds_LR.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id    Pred_LR\n",
       "0  10  11.930557\n",
       "1  13  11.768501\n",
       "2  14  12.025407\n",
       "3  31  12.040083\n",
       "4  37  11.899383"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_LR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>11.930557</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13</td>\n      <td>11.768501</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14</td>\n      <td>12.025407</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31</td>\n      <td>12.040083</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37</td>\n      <td>11.899383</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "va_idxes_LR = np.concatenate(va_idxes_LR)\n",
    "va_pred_LR = np.concatenate(va_preds_LR,axis = 0)\n",
    "#order = np.argsort(va_idxes_LR)\n",
    "va_pred_LR = pd.DataFrame(data = va_pred_LR)\n",
    "order       = pd.DataFrame(data = va_idxes_LR)\n",
    "pred_train_LR = pd.concat([order,va_pred_LR],axis = 1)\n",
    "pred_train_LR.columns = [\"id\",\"Pred_LR\"]\n",
    "\n",
    "preds_test_LR = np.mean(preds_LR, axis = 0)\n",
    "\n",
    "pred_train_LR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\nTabNet\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "TabNet\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/ishikawa/signate/home_price/train.csv\")\n",
    "test  = pd.read_csv(\"/home/ishikawa/signate/home_price/test.csv\")\n",
    "\n",
    "#train = train[train['SalePrice'] < 200000]\n",
    "\n",
    "category = train.columns[train.dtypes == \"object\"]\n",
    "all_x = pd.concat([train, test])\n",
    "all_x_one = pd.get_dummies(all_x, columns = category)\n",
    "\n",
    "train_one = all_x_one.iloc[:train.shape[0], :].reset_index(drop = True)\n",
    "test_one  = all_x_one.iloc[train.shape[0]:, :].reset_index(drop = True)\n",
    "\n",
    "train = pd.DataFrame(data = train_one)\n",
    "test = pd.DataFrame(data = test_one)\n",
    "\n",
    "del train_one\n",
    "del test_one\n",
    "\n",
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "\n",
    "train = train.drop(columns=['Garage Area'])\n",
    "test = test.drop(columns=[ 'Garage Area'])\n",
    "\n",
    "#train['Total_Bsmt'] = train['BsmtFin SF 1'] + train['Bsmt Unf SF']\n",
    "#test['Total_Bsmt'] = test['BsmtFin SF 1'] + test['Bsmt Unf SF']\n",
    "\n",
    "#train['Total Bsmt SF'] = train['Total Bsmt SF'].where(train['Total Bsmt SF'] > 0, train['Total_Bsmt'])\n",
    "#test['Total Bsmt SF'] = test['Total Bsmt SF'].where(test['Total Bsmt SF'] > 0, test['Total_Bsmt'])\n",
    "\n",
    "#train = train.drop(columns=['Bsmt Full Bath','Year Remod/Add'])\n",
    "#test = test.drop(columns=[ 'Bsmt Full Bath','Year Remod/Add'])\n",
    "\n",
    "#train = train.drop(columns=['Total_Bsmt'])\n",
    "#test = test.drop(columns=[ 'Total_Bsmt'])\n",
    "\n",
    "train_y = pd.DataFrame(train['SalePrice'])\n",
    "del train['SalePrice']\n",
    "train_x = pd.DataFrame(data = train)\n",
    "del train\n",
    "\n",
    "del test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 291705467.69231| val_0_unsup_loss: 127520520.0|  0:00:00s\n",
      "epoch 10 | loss: 97570.64123| val_0_unsup_loss: 66774.16406|  0:00:04s\n",
      "epoch 20 | loss: 625781.71184| val_0_unsup_loss: 100130.44531|  0:00:08s\n",
      "epoch 30 | loss: 99660.9357| val_0_unsup_loss: 40729.91016|  0:00:11s\n",
      "epoch 40 | loss: 76886.40835| val_0_unsup_loss: 40388.91016|  0:00:15s\n",
      "epoch 50 | loss: 46859.76337| val_0_unsup_loss: 133626.39062|  0:00:18s\n",
      "epoch 60 | loss: 39527.21785| val_0_unsup_loss: 32096.03516|  0:00:22s\n",
      "epoch 70 | loss: 38879.08203| val_0_unsup_loss: 33220.35938|  0:00:25s\n",
      "epoch 80 | loss: 35451.77524| val_0_unsup_loss: 31514.89648|  0:00:29s\n",
      "epoch 90 | loss: 35228.23227| val_0_unsup_loss: 30432.14453|  0:00:33s\n",
      "\n",
      "Early stopping occured at epoch 91 with best_epoch = 71 and best_val_0_unsup_loss = 29641.92969\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 143.66849| valid_rmse: 11.9744 |  0:00:00s\n",
      "epoch 10 | loss: 22.65344| valid_rmse: 5.24143 |  0:00:03s\n",
      "epoch 20 | loss: 0.21047 | valid_rmse: 0.81148 |  0:00:07s\n",
      "epoch 30 | loss: 0.08459 | valid_rmse: 0.2616  |  0:00:10s\n",
      "epoch 40 | loss: 0.06608 | valid_rmse: 0.29949 |  0:00:13s\n",
      "epoch 50 | loss: 0.0505  | valid_rmse: 0.25825 |  0:00:16s\n",
      "epoch 60 | loss: 0.06613 | valid_rmse: 0.25505 |  0:00:20s\n",
      "epoch 70 | loss: 0.05327 | valid_rmse: 0.40786 |  0:00:23s\n",
      "epoch 80 | loss: 0.07622 | valid_rmse: 0.24364 |  0:00:26s\n",
      "\n",
      "Early stopping occured at epoch 82 with best_epoch = 62 and best_valid_rmse = 0.20047\n",
      "Best weights from best epoch are automatically used!\n",
      "0.20047338748956586\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 295225310.76923| val_0_unsup_loss: 352893728.0|  0:00:00s\n",
      "epoch 10 | loss: 288892.95282| val_0_unsup_loss: 192469.39062|  0:00:04s\n",
      "epoch 20 | loss: 154436.9396| val_0_unsup_loss: 184376.51562|  0:00:07s\n",
      "epoch 30 | loss: 61692.06971| val_0_unsup_loss: 44882.89453|  0:00:11s\n",
      "epoch 40 | loss: 71995.73918| val_0_unsup_loss: 53207.34375|  0:00:14s\n",
      "epoch 50 | loss: 79418.65655| val_0_unsup_loss: 45791.84375|  0:00:18s\n",
      "\n",
      "Early stopping occured at epoch 54 with best_epoch = 34 and best_val_0_unsup_loss = 30730.11914\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 134.76798| valid_rmse: 11.67497|  0:00:00s\n",
      "epoch 10 | loss: 2.47207 | valid_rmse: 3.14685 |  0:00:03s\n",
      "epoch 20 | loss: 0.32317 | valid_rmse: 0.65338 |  0:00:07s\n",
      "epoch 30 | loss: 0.12451 | valid_rmse: 0.31822 |  0:00:10s\n",
      "epoch 40 | loss: 0.15478 | valid_rmse: 0.68729 |  0:00:13s\n",
      "epoch 50 | loss: 0.07291 | valid_rmse: 0.59063 |  0:00:17s\n",
      "epoch 60 | loss: 0.08587 | valid_rmse: 0.31732 |  0:00:20s\n",
      "epoch 70 | loss: 0.05893 | valid_rmse: 4.19307 |  0:00:23s\n",
      "epoch 80 | loss: 0.15427 | valid_rmse: 0.4765  |  0:00:27s\n",
      "\n",
      "Early stopping occured at epoch 85 with best_epoch = 65 and best_valid_rmse = 0.15484\n",
      "Best weights from best epoch are automatically used!\n",
      "0.154838357374885\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 289394310.76923| val_0_unsup_loss: 1356211072.0|  0:00:00s\n",
      "epoch 10 | loss: 164631.52734| val_0_unsup_loss: 58780.83203|  0:00:04s\n",
      "epoch 20 | loss: 115603.5625| val_0_unsup_loss: 73312.17188|  0:00:07s\n",
      "epoch 30 | loss: 62189.814| val_0_unsup_loss: 44042.80859|  0:00:11s\n",
      "epoch 40 | loss: 44280.05499| val_0_unsup_loss: 52730.25|  0:00:14s\n",
      "epoch 50 | loss: 43584.15219| val_0_unsup_loss: 34148.66016|  0:00:18s\n",
      "\n",
      "Early stopping occured at epoch 53 with best_epoch = 33 and best_val_0_unsup_loss = 33140.92969\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 139.07157| valid_rmse: 11.80554|  0:00:00s\n",
      "epoch 10 | loss: 6.15993 | valid_rmse: 2.85524 |  0:00:03s\n",
      "epoch 20 | loss: 0.11647 | valid_rmse: 1.28953 |  0:00:07s\n",
      "epoch 30 | loss: 0.19697 | valid_rmse: 0.53701 |  0:00:10s\n",
      "epoch 40 | loss: 0.06761 | valid_rmse: 0.38109 |  0:00:13s\n",
      "epoch 50 | loss: 0.06726 | valid_rmse: 0.26981 |  0:00:17s\n",
      "epoch 60 | loss: 0.06948 | valid_rmse: 0.26782 |  0:00:20s\n",
      "\n",
      "Early stopping occured at epoch 68 with best_epoch = 48 and best_valid_rmse = 0.2023\n",
      "Best weights from best epoch are automatically used!\n",
      "0.20229641664634251\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 271358896.61538| val_0_unsup_loss: 7109980160.0|  0:00:00s\n",
      "epoch 10 | loss: 159053.09014| val_0_unsup_loss: 333184.1875|  0:00:04s\n",
      "epoch 20 | loss: 67941.32512| val_0_unsup_loss: 156243.375|  0:00:07s\n",
      "epoch 30 | loss: 55782.22746| val_0_unsup_loss: 56874.59766|  0:00:11s\n",
      "epoch 40 | loss: 59693.1232| val_0_unsup_loss: 47065.37891|  0:00:14s\n",
      "epoch 50 | loss: 46378.74159| val_0_unsup_loss: 40975.27734|  0:00:18s\n",
      "\n",
      "Early stopping occured at epoch 57 with best_epoch = 37 and best_val_0_unsup_loss = 36044.91016\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 143.37719| valid_rmse: 11.99191|  0:00:00s\n",
      "epoch 10 | loss: 15.00073| valid_rmse: 3.91168 |  0:00:03s\n",
      "epoch 20 | loss: 0.4146  | valid_rmse: 0.5123  |  0:00:07s\n",
      "epoch 30 | loss: 0.15307 | valid_rmse: 0.53134 |  0:00:10s\n",
      "epoch 40 | loss: 0.37817 | valid_rmse: 0.41918 |  0:00:14s\n",
      "epoch 50 | loss: 0.05713 | valid_rmse: 0.25111 |  0:00:17s\n",
      "epoch 60 | loss: 0.08446 | valid_rmse: 0.24061 |  0:00:20s\n",
      "epoch 70 | loss: 0.10516 | valid_rmse: 1.42921 |  0:00:24s\n",
      "epoch 80 | loss: 0.13639 | valid_rmse: 0.27231 |  0:00:27s\n",
      "epoch 90 | loss: 0.1501  | valid_rmse: 0.4238  |  0:00:30s\n",
      "epoch 100| loss: 0.09324 | valid_rmse: 0.27082 |  0:00:34s\n",
      "epoch 110| loss: 0.07715 | valid_rmse: 0.2253  |  0:00:37s\n",
      "\n",
      "Early stopping occured at epoch 119 with best_epoch = 99 and best_valid_rmse = 0.17894\n",
      "Best weights from best epoch are automatically used!\n",
      "0.17894420216341478\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 315779510.76923| val_0_unsup_loss: 367091136.0|  0:00:00s\n",
      "epoch 10 | loss: 174650.21905| val_0_unsup_loss: 75171.33594|  0:00:04s\n",
      "epoch 20 | loss: 68491.52043| val_0_unsup_loss: 580252.5625|  0:00:07s\n",
      "epoch 30 | loss: 123248.42698| val_0_unsup_loss: 46091.39062|  0:00:11s\n",
      "epoch 40 | loss: 164499.20703| val_0_unsup_loss: 46064.39453|  0:00:14s\n",
      "epoch 50 | loss: 45061.36959| val_0_unsup_loss: 75099.125|  0:00:18s\n",
      "epoch 60 | loss: 98769.16106| val_0_unsup_loss: 61019.01953|  0:00:22s\n",
      "epoch 70 | loss: 36886.49354| val_0_unsup_loss: 38011.42969|  0:00:25s\n",
      "epoch 80 | loss: 35691.31671| val_0_unsup_loss: 62710.57031|  0:00:29s\n",
      "epoch 90 | loss: 34530.55934| val_0_unsup_loss: 40673.625|  0:00:33s\n",
      "\n",
      "Early stopping occured at epoch 91 with best_epoch = 71 and best_val_0_unsup_loss = 37284.4375\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 140.09133| valid_rmse: 11.81448|  0:00:00s\n",
      "epoch 10 | loss: 8.52354 | valid_rmse: 3.52806 |  0:00:03s\n",
      "epoch 20 | loss: 0.17666 | valid_rmse: 0.91688 |  0:00:07s\n",
      "epoch 30 | loss: 0.09236 | valid_rmse: 0.28681 |  0:00:10s\n",
      "epoch 40 | loss: 0.06103 | valid_rmse: 0.31034 |  0:00:13s\n",
      "epoch 50 | loss: 0.06758 | valid_rmse: 0.2848  |  0:00:17s\n",
      "\n",
      "Early stopping occured at epoch 55 with best_epoch = 35 and best_valid_rmse = 0.19881\n",
      "Best weights from best epoch are automatically used!\n",
      "0.19880814306577058\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 301807102.15385| val_0_unsup_loss: 362318720.0|  0:00:00s\n",
      "epoch 10 | loss: 246918.3155| val_0_unsup_loss: 171079.46875|  0:00:04s\n",
      "epoch 20 | loss: 128100.17758| val_0_unsup_loss: 47324.80078|  0:00:07s\n",
      "epoch 30 | loss: 87666.1259| val_0_unsup_loss: 53631.03906|  0:00:11s\n",
      "epoch 40 | loss: 49472.68915| val_0_unsup_loss: 81523.67969|  0:00:14s\n",
      "epoch 50 | loss: 44702.53425| val_0_unsup_loss: 36163.37109|  0:00:18s\n",
      "epoch 60 | loss: 48589.28726| val_0_unsup_loss: 40671.86719|  0:00:22s\n",
      "epoch 70 | loss: 45226.02524| val_0_unsup_loss: 190046.5625|  0:00:25s\n",
      "epoch 80 | loss: 52099.79853| val_0_unsup_loss: 178586.73438|  0:00:29s\n",
      "epoch 90 | loss: 54413.09841| val_0_unsup_loss: 60947.24219|  0:00:33s\n",
      "epoch 100| loss: 34852.45343| val_0_unsup_loss: 43768.89844|  0:00:36s\n",
      "epoch 110| loss: 32606.30619| val_0_unsup_loss: 36831.4375|  0:00:40s\n",
      "epoch 120| loss: 37167.6887| val_0_unsup_loss: 35555.58594|  0:00:44s\n",
      "epoch 130| loss: 33565.40655| val_0_unsup_loss: 35337.30469|  0:00:47s\n",
      "epoch 140| loss: 34544.64002| val_0_unsup_loss: 50538.32812|  0:00:51s\n",
      "epoch 150| loss: 31904.12124| val_0_unsup_loss: 48916.67969|  0:00:54s\n",
      "\n",
      "Early stopping occured at epoch 159 with best_epoch = 139 and best_val_0_unsup_loss = 30312.42969\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 138.532 | valid_rmse: 11.7766 |  0:00:00s\n",
      "epoch 10 | loss: 22.57284| valid_rmse: 4.01614 |  0:00:03s\n",
      "epoch 20 | loss: 0.31943 | valid_rmse: 0.3053  |  0:00:07s\n",
      "epoch 30 | loss: 0.08295 | valid_rmse: 0.52525 |  0:00:10s\n",
      "epoch 40 | loss: 0.05767 | valid_rmse: 0.27984 |  0:00:14s\n",
      "epoch 50 | loss: 0.06079 | valid_rmse: 0.39531 |  0:00:17s\n",
      "\n",
      "Early stopping occured at epoch 59 with best_epoch = 39 and best_valid_rmse = 0.19528\n",
      "Best weights from best epoch are automatically used!\n",
      "0.19527863022504915\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 337399355.07692| val_0_unsup_loss: 422472318976.0|  0:00:00s\n",
      "epoch 10 | loss: 221132.07572| val_0_unsup_loss: 252796.21875|  0:00:04s\n",
      "epoch 20 | loss: 224732.08684| val_0_unsup_loss: 90300.29688|  0:00:07s\n",
      "epoch 30 | loss: 85563.40505| val_0_unsup_loss: 43990.88281|  0:00:11s\n",
      "epoch 40 | loss: 77016.92608| val_0_unsup_loss: 45708.19531|  0:00:14s\n",
      "epoch 50 | loss: 43557.83128| val_0_unsup_loss: 56614.91797|  0:00:18s\n",
      "epoch 60 | loss: 43329.89123| val_0_unsup_loss: 47859.875|  0:00:22s\n",
      "\n",
      "Early stopping occured at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss = 38145.05078\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 140.62414| valid_rmse: 11.8218 |  0:00:00s\n",
      "epoch 10 | loss: 9.44999 | valid_rmse: 4.45524 |  0:00:03s\n",
      "epoch 20 | loss: 0.20668 | valid_rmse: 0.63835 |  0:00:07s\n",
      "epoch 30 | loss: 0.07457 | valid_rmse: 0.21914 |  0:00:10s\n",
      "epoch 40 | loss: 0.0747  | valid_rmse: 0.26703 |  0:00:13s\n",
      "epoch 50 | loss: 0.0658  | valid_rmse: 0.20143 |  0:00:17s\n",
      "epoch 60 | loss: 0.04832 | valid_rmse: 0.3342  |  0:00:20s\n",
      "\n",
      "Early stopping occured at epoch 61 with best_epoch = 41 and best_valid_rmse = 0.16424\n",
      "Best weights from best epoch are automatically used!\n",
      "0.16424039667419021\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 301313874.46154| val_0_unsup_loss: 3423813888.0|  0:00:00s\n",
      "epoch 10 | loss: 128156.38161| val_0_unsup_loss: 182395.4375|  0:00:04s\n",
      "epoch 20 | loss: 258720.11148| val_0_unsup_loss: 195032.67188|  0:00:07s\n",
      "epoch 30 | loss: 59111.14844| val_0_unsup_loss: 57839.67969|  0:00:11s\n",
      "epoch 40 | loss: 44229.7207| val_0_unsup_loss: 78606.94531|  0:00:14s\n",
      "epoch 50 | loss: 52887.39123| val_0_unsup_loss: 67656.71094|  0:00:18s\n",
      "epoch 60 | loss: 42274.88191| val_0_unsup_loss: 46497.55469|  0:00:22s\n",
      "epoch 70 | loss: 45321.23347| val_0_unsup_loss: 366383.9375|  0:00:25s\n",
      "epoch 80 | loss: 35983.86253| val_0_unsup_loss: 63875.38281|  0:00:29s\n",
      "epoch 90 | loss: 36858.67293| val_0_unsup_loss: 37819.30859|  0:00:32s\n",
      "epoch 100| loss: 35169.68164| val_0_unsup_loss: 42040.0 |  0:00:36s\n",
      "epoch 110| loss: 33589.79147| val_0_unsup_loss: 39554.98438|  0:00:39s\n",
      "\n",
      "Early stopping occured at epoch 113 with best_epoch = 93 and best_val_0_unsup_loss = 34920.45312\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 141.12563| valid_rmse: 11.78048|  0:00:00s\n",
      "epoch 10 | loss: 24.85397| valid_rmse: 3.64295 |  0:00:03s\n",
      "epoch 20 | loss: 0.14666 | valid_rmse: 0.76826 |  0:00:07s\n",
      "epoch 30 | loss: 0.14264 | valid_rmse: 0.43342 |  0:00:10s\n",
      "epoch 40 | loss: 0.12888 | valid_rmse: 0.37013 |  0:00:13s\n",
      "epoch 50 | loss: 0.08086 | valid_rmse: 0.19891 |  0:00:16s\n",
      "epoch 60 | loss: 0.08629 | valid_rmse: 0.30037 |  0:00:20s\n",
      "epoch 70 | loss: 0.04833 | valid_rmse: 0.23228 |  0:00:23s\n",
      "epoch 80 | loss: 0.0793  | valid_rmse: 0.24272 |  0:00:26s\n",
      "\n",
      "Early stopping occured at epoch 87 with best_epoch = 67 and best_valid_rmse = 0.16662\n",
      "Best weights from best epoch are automatically used!\n",
      "0.1666179178398135\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 312810548.92308| val_0_unsup_loss: 1460035200.0|  0:00:00s\n",
      "epoch 10 | loss: 293472.54207| val_0_unsup_loss: 204596.64062|  0:00:04s\n",
      "epoch 20 | loss: 105569.85847| val_0_unsup_loss: 471309.46875|  0:00:07s\n",
      "epoch 30 | loss: 62747.75661| val_0_unsup_loss: 71547.58594|  0:00:11s\n",
      "epoch 40 | loss: 59823.76668| val_0_unsup_loss: 35179.25|  0:00:14s\n",
      "epoch 50 | loss: 46500.76472| val_0_unsup_loss: 34124.76562|  0:00:18s\n",
      "epoch 60 | loss: 45953.55634| val_0_unsup_loss: 57737.86328|  0:00:22s\n",
      "epoch 70 | loss: 84072.20358| val_0_unsup_loss: 63186.91797|  0:00:25s\n",
      "epoch 80 | loss: 38180.89994| val_0_unsup_loss: 36775.85156|  0:00:29s\n",
      "\n",
      "Early stopping occured at epoch 81 with best_epoch = 61 and best_val_0_unsup_loss = 31253.30664\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 146.55524| valid_rmse: 12.04581|  0:00:00s\n",
      "epoch 10 | loss: 5.90568 | valid_rmse: 2.79779 |  0:00:03s\n",
      "epoch 20 | loss: 0.14309 | valid_rmse: 0.28787 |  0:00:07s\n",
      "epoch 30 | loss: 0.11819 | valid_rmse: 0.3656  |  0:00:10s\n",
      "epoch 40 | loss: 0.06859 | valid_rmse: 0.32492 |  0:00:13s\n",
      "epoch 50 | loss: 0.07159 | valid_rmse: 0.26034 |  0:00:17s\n",
      "\n",
      "Early stopping occured at epoch 52 with best_epoch = 32 and best_valid_rmse = 0.2406\n",
      "Best weights from best epoch are automatically used!\n",
      "0.2406033287941195\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 317077183.38462| val_0_unsup_loss: 106631920.0|  0:00:00s\n",
      "epoch 10 | loss: 383763.59014| val_0_unsup_loss: 161247.28125|  0:00:04s\n",
      "epoch 20 | loss: 86760.25631| val_0_unsup_loss: 131524.60938|  0:00:07s\n",
      "epoch 30 | loss: 57195.96785| val_0_unsup_loss: 90943.32031|  0:00:11s\n",
      "epoch 40 | loss: 66089.95643| val_0_unsup_loss: 49072.48047|  0:00:15s\n",
      "\n",
      "Early stopping occured at epoch 46 with best_epoch = 26 and best_val_0_unsup_loss = 37663.91797\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 147.01551| valid_rmse: 12.10971|  0:00:00s\n",
      "epoch 10 | loss: 14.3268 | valid_rmse: 3.2906  |  0:00:03s\n",
      "epoch 20 | loss: 0.17296 | valid_rmse: 0.39649 |  0:00:07s\n",
      "epoch 30 | loss: 0.10384 | valid_rmse: 0.2556  |  0:00:10s\n",
      "epoch 40 | loss: 0.06911 | valid_rmse: 0.64414 |  0:00:13s\n",
      "epoch 50 | loss: 0.06516 | valid_rmse: 0.25603 |  0:00:17s\n",
      "epoch 60 | loss: 0.05582 | valid_rmse: 0.20633 |  0:00:20s\n",
      "epoch 70 | loss: 0.07963 | valid_rmse: 0.48049 |  0:00:24s\n",
      "epoch 80 | loss: 0.07718 | valid_rmse: 0.25579 |  0:00:27s\n",
      "\n",
      "Early stopping occured at epoch 82 with best_epoch = 62 and best_valid_rmse = 0.18534\n",
      "Best weights from best epoch are automatically used!\n",
      "0.18533794982307517\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 1223)\n",
    "\n",
    "preds_TN = []\n",
    "rmses = []\n",
    "va_idxes_TN = []\n",
    "va_preds_TN = []\n",
    "\n",
    "\n",
    "for tr_idx,va_idx in kfold.split(train_x):\n",
    "    \n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    tabnet_params = dict(n_d=50, n_a=50, n_steps=3, gamma=1.3,\n",
    "                         n_independent=2, n_shared=2,\n",
    "                         seed=1223, lambda_sparse=1e-3, \n",
    "                         optimizer_fn=torch.optim.Adam, \n",
    "                         optimizer_params=dict(lr=2e-2),\n",
    "                         mask_type=\"entmax\",\n",
    "                         scheduler_params=dict(mode=\"min\",\n",
    "                                               patience=5,\n",
    "                                               min_lr=1e-5,\n",
    "                                               factor=0.9,),\n",
    "                         scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                         verbose=10\n",
    "                         )\n",
    "\n",
    "    pretrainer = TabNetPretrainer(**tabnet_params)\n",
    "\n",
    "    pretrainer.fit(\n",
    "                    X_train=tr_x.values,\n",
    "                    eval_set=[va_x.values],\n",
    "                    max_epochs=200,\n",
    "                    patience=20, batch_size=200, virtual_batch_size=100,\n",
    "                    num_workers=0, drop_last=True,pretraining_ratio=0.8)\n",
    "    \n",
    "    tabnet_params = dict(n_d=50, n_a=50, n_steps=3, gamma=1.3,\n",
    "                         n_independent=2, n_shared=2,\n",
    "                         seed=1223, lambda_sparse=1e-3,\n",
    "                         optimizer_fn=torch.optim.Adam,\n",
    "                         optimizer_params=dict(lr=2e-2,\n",
    "                                               weight_decay=1e-5\n",
    "                                              ),\n",
    "                         mask_type=\"entmax\",\n",
    "                         scheduler_params=dict(max_lr=0.05,\n",
    "                                               steps_per_epoch=int(tr_x.shape[0] /100),\n",
    "                                               epochs=200,\n",
    "                                               is_batch_level=True\n",
    "                                              ),\n",
    "                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n",
    "                         verbose=10,\n",
    "                         #cat_idxs=cat_idxs, # comment out when Unsupervised\n",
    "                         #cat_dims=cat_dims, # comment out when Unsupervised\n",
    "                         #cat_emb_dim=1 # comment out when Unsupervised\n",
    "                        )\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    model.fit(X_train=tr_x.values,\n",
    "                  y_train=tr_y.values,\n",
    "                  eval_set=[(va_x.values, va_y.values)],\n",
    "                  eval_name = [\"valid\"],\n",
    "                  eval_metric = [\"rmse\"],\n",
    "                  max_epochs=1000,\n",
    "                  patience=20, batch_size=200, virtual_batch_size=100,\n",
    "                  num_workers=0, drop_last=False,\n",
    "                  from_unsupervised=pretrainer # comment out when Unsupervised\n",
    "                 )\n",
    "    \n",
    "    va_pred = model.predict(va_x.values)\n",
    "    va_preds_TN.append(va_pred)\n",
    "    \n",
    "    va_idxes_TN.append(va_idx)\n",
    "    \n",
    "    pred = model.predict(test.values)\n",
    "    preds_TN.append(pred)\n",
    "    \n",
    "    tmp_rmse = np.sqrt(mean_squared_error(va_pred, va_y))\n",
    "    rmses.append(tmp_rmse)\n",
    "    print(tmp_rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id    Pred_TN\n",
       "0   1  11.929918\n",
       "1   7  11.796266\n",
       "2  10  11.932304\n",
       "3  16  11.927071\n",
       "4  23  11.961583"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_TN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11.929918</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>11.796266</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>11.932304</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>11.927071</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>11.961583</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "va_idxes_TN = np.concatenate(va_idxes_TN)\n",
    "va_pred_TN = np.concatenate(va_preds_TN,axis = 0)\n",
    "va_pred_TN = pd.DataFrame(data = va_pred_TN)\n",
    "order       = pd.DataFrame(data = va_idxes_TN)\n",
    "pred_train_TN = pd.concat([order,va_pred_TN],axis = 1)\n",
    "pred_train_TN.columns = [\"id\",\"Pred_TN\"]\n",
    "\n",
    "preds_test_TN = np.mean(preds_TN, axis = 0)\n",
    "\n",
    "pred_train_TN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id    Pred_TN\n",
       "2995  2899  11.823657\n",
       "2996  2901  11.890195\n",
       "2997  2979  11.744009\n",
       "2998  2982  11.810146\n",
       "2999  2995  11.839073"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_TN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2995</th>\n      <td>2899</td>\n      <td>11.823657</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>2901</td>\n      <td>11.890195</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>2979</td>\n      <td>11.744009</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>2982</td>\n      <td>11.810146</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>2995</td>\n      <td>11.839073</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "pred_train_TN.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'XGboost'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "'''XGboost'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/ishikawa/signate/home_price/train.csv\")\n",
    "test  = pd.read_csv(\"/home/ishikawa/signate/home_price/test.csv\")\n",
    "\n",
    "#train = train[train['SalePrice'] < 200000]\n",
    "\n",
    "category = train.columns[train.dtypes == \"object\"]\n",
    "all_x = pd.concat([train, test])\n",
    "all_x_one = pd.get_dummies(all_x, columns = category)\n",
    "\n",
    "train_one = all_x_one.iloc[:train.shape[0], :].reset_index(drop = True)\n",
    "test_one  = all_x_one.iloc[train.shape[0]:, :].reset_index(drop = True)\n",
    "\n",
    "train = pd.DataFrame(data = train_one)\n",
    "test = pd.DataFrame(data = test_one)\n",
    "\n",
    "del train_one\n",
    "del test_one\n",
    "\n",
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "\n",
    "train = train.drop(columns=['Garage Area'])\n",
    "test = test.drop(columns=[ 'Garage Area'])\n",
    "\n",
    "#train['Total_Bsmt'] = train['BsmtFin SF 1'] + train['Bsmt Unf SF']\n",
    "#test['Total_Bsmt'] = test['BsmtFin SF 1'] + test['Bsmt Unf SF']\n",
    "\n",
    "#train['Total Bsmt SF'] = train['Total Bsmt SF'].where(train['Total Bsmt SF'] > 0, train['Total_Bsmt'])\n",
    "#test['Total Bsmt SF'] = test['Total Bsmt SF'].where(test['Total Bsmt SF'] > 0, test['Total_Bsmt'])\n",
    "\n",
    "#train = train.drop(columns=['Bsmt Full Bath','Year Remod/Add'])\n",
    "#test = test.drop(columns=[ 'Bsmt Full Bath','Year Remod/Add'])\n",
    "\n",
    "#train = train.drop(columns=['Total_Bsmt'])\n",
    "#test = test.drop(columns=[ 'Total_Bsmt'])\n",
    "\n",
    "train_y = pd.DataFrame(train['SalePrice'])\n",
    "del train['SalePrice']\n",
    "train_x = pd.DataFrame(data = train)\n",
    "del train\n",
    "\n",
    "del test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFold = KFold(n_splits = 10, shuffle = True, random_state = 1223)\n",
    "xgb_params = {\"eta\":0.01,\n",
    "               \"seed\":2020,\n",
    "                \"max_depth\":20,\n",
    "                'objective':'reg:squarederror',\n",
    "                'eval_metric':'rmse',\n",
    "                'tree_method':'gpu_hist' ,\n",
    "                'lambda':3\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "393\n",
      "[1546]\ttrain-rmse:0.00234\teval-rmse:0.16393\n",
      "[1547]\ttrain-rmse:0.00234\teval-rmse:0.16393\n",
      "[1548]\ttrain-rmse:0.00233\teval-rmse:0.16393\n",
      "[1549]\ttrain-rmse:0.00233\teval-rmse:0.16393\n",
      "[1550]\ttrain-rmse:0.00232\teval-rmse:0.16393\n",
      "[1551]\ttrain-rmse:0.00231\teval-rmse:0.16393\n",
      "[1552]\ttrain-rmse:0.00231\teval-rmse:0.16393\n",
      "[1553]\ttrain-rmse:0.00230\teval-rmse:0.16393\n",
      "[1554]\ttrain-rmse:0.00230\teval-rmse:0.16393\n",
      "[1555]\ttrain-rmse:0.00229\teval-rmse:0.16393\n",
      "[1556]\ttrain-rmse:0.00229\teval-rmse:0.16393\n",
      "[1557]\ttrain-rmse:0.00228\teval-rmse:0.16393\n",
      "[1558]\ttrain-rmse:0.00227\teval-rmse:0.16393\n",
      "[1559]\ttrain-rmse:0.00227\teval-rmse:0.16393\n",
      "[1560]\ttrain-rmse:0.00226\teval-rmse:0.16393\n",
      "[1561]\ttrain-rmse:0.00226\teval-rmse:0.16393\n",
      "[1562]\ttrain-rmse:0.00225\teval-rmse:0.16393\n",
      "[1563]\ttrain-rmse:0.00224\teval-rmse:0.16393\n",
      "[1564]\ttrain-rmse:0.00224\teval-rmse:0.16393\n",
      "[1565]\ttrain-rmse:0.00223\teval-rmse:0.16393\n",
      "[1566]\ttrain-rmse:0.00223\teval-rmse:0.16394\n",
      "[1567]\ttrain-rmse:0.00222\teval-rmse:0.16394\n",
      "[1568]\ttrain-rmse:0.00221\teval-rmse:0.16394\n",
      "[1569]\ttrain-rmse:0.00221\teval-rmse:0.16394\n",
      "[1570]\ttrain-rmse:0.00220\teval-rmse:0.16394\n",
      "[1571]\ttrain-rmse:0.00220\teval-rmse:0.16394\n",
      "[1572]\ttrain-rmse:0.00219\teval-rmse:0.16394\n",
      "[1573]\ttrain-rmse:0.00219\teval-rmse:0.16394\n",
      "[1574]\ttrain-rmse:0.00218\teval-rmse:0.16394\n",
      "[1575]\ttrain-rmse:0.00218\teval-rmse:0.16394\n",
      "[1576]\ttrain-rmse:0.00217\teval-rmse:0.16394\n",
      "[1577]\ttrain-rmse:0.00216\teval-rmse:0.16394\n",
      "[1578]\ttrain-rmse:0.00216\teval-rmse:0.16394\n",
      "[1579]\ttrain-rmse:0.00215\teval-rmse:0.16394\n",
      "[1580]\ttrain-rmse:0.00215\teval-rmse:0.16394\n",
      "[1581]\ttrain-rmse:0.00214\teval-rmse:0.16394\n",
      "[1582]\ttrain-rmse:0.00214\teval-rmse:0.16395\n",
      "[1583]\ttrain-rmse:0.00213\teval-rmse:0.16395\n",
      "[1584]\ttrain-rmse:0.00213\teval-rmse:0.16395\n",
      "[1585]\ttrain-rmse:0.00212\teval-rmse:0.16395\n",
      "[1586]\ttrain-rmse:0.00212\teval-rmse:0.16395\n",
      "[1587]\ttrain-rmse:0.00211\teval-rmse:0.16395\n",
      "[1588]\ttrain-rmse:0.00211\teval-rmse:0.16395\n",
      "[1589]\ttrain-rmse:0.00210\teval-rmse:0.16395\n",
      "[1590]\ttrain-rmse:0.00210\teval-rmse:0.16395\n",
      "[1591]\ttrain-rmse:0.00209\teval-rmse:0.16395\n",
      "[1592]\ttrain-rmse:0.00209\teval-rmse:0.16395\n",
      "[1593]\ttrain-rmse:0.00208\teval-rmse:0.16395\n",
      "[1594]\ttrain-rmse:0.00207\teval-rmse:0.16395\n",
      "[1595]\ttrain-rmse:0.00207\teval-rmse:0.16395\n",
      "[1596]\ttrain-rmse:0.00206\teval-rmse:0.16395\n",
      "[1597]\ttrain-rmse:0.00206\teval-rmse:0.16395\n",
      "[1598]\ttrain-rmse:0.00205\teval-rmse:0.16395\n",
      "[1599]\ttrain-rmse:0.00205\teval-rmse:0.16395\n",
      "[1600]\ttrain-rmse:0.00204\teval-rmse:0.16395\n",
      "[1601]\ttrain-rmse:0.00204\teval-rmse:0.16395\n",
      "[1602]\ttrain-rmse:0.00203\teval-rmse:0.16395\n",
      "[1603]\ttrain-rmse:0.00203\teval-rmse:0.16395\n",
      "[1604]\ttrain-rmse:0.00202\teval-rmse:0.16395\n",
      "[1605]\ttrain-rmse:0.00202\teval-rmse:0.16396\n",
      "[1606]\ttrain-rmse:0.00201\teval-rmse:0.16396\n",
      "[1607]\ttrain-rmse:0.00201\teval-rmse:0.16396\n",
      "[1608]\ttrain-rmse:0.00200\teval-rmse:0.16396\n",
      "[1609]\ttrain-rmse:0.00200\teval-rmse:0.16396\n",
      "[1610]\ttrain-rmse:0.00199\teval-rmse:0.16396\n",
      "[1611]\ttrain-rmse:0.00199\teval-rmse:0.16396\n",
      "[1612]\ttrain-rmse:0.00198\teval-rmse:0.16396\n",
      "[1613]\ttrain-rmse:0.00198\teval-rmse:0.16396\n",
      "[1614]\ttrain-rmse:0.00198\teval-rmse:0.16396\n",
      "[1615]\ttrain-rmse:0.00197\teval-rmse:0.16396\n",
      "[1616]\ttrain-rmse:0.00197\teval-rmse:0.16396\n",
      "[1617]\ttrain-rmse:0.00196\teval-rmse:0.16396\n",
      "[1618]\ttrain-rmse:0.00196\teval-rmse:0.16396\n",
      "[1619]\ttrain-rmse:0.00195\teval-rmse:0.16396\n",
      "[1620]\ttrain-rmse:0.00195\teval-rmse:0.16396\n",
      "[1621]\ttrain-rmse:0.00194\teval-rmse:0.16396\n",
      "[1622]\ttrain-rmse:0.00194\teval-rmse:0.16396\n",
      "[1623]\ttrain-rmse:0.00193\teval-rmse:0.16397\n",
      "[1624]\ttrain-rmse:0.00193\teval-rmse:0.16397\n",
      "[1625]\ttrain-rmse:0.00192\teval-rmse:0.16397\n",
      "[1626]\ttrain-rmse:0.00192\teval-rmse:0.16397\n",
      "[1627]\ttrain-rmse:0.00191\teval-rmse:0.16397\n",
      "[1628]\ttrain-rmse:0.00191\teval-rmse:0.16397\n",
      "[1629]\ttrain-rmse:0.00190\teval-rmse:0.16397\n",
      "[1630]\ttrain-rmse:0.00190\teval-rmse:0.16397\n",
      "[1631]\ttrain-rmse:0.00189\teval-rmse:0.16397\n",
      "[1632]\ttrain-rmse:0.00189\teval-rmse:0.16397\n",
      "[1633]\ttrain-rmse:0.00188\teval-rmse:0.16397\n",
      "[1634]\ttrain-rmse:0.00188\teval-rmse:0.16397\n",
      "[1635]\ttrain-rmse:0.00187\teval-rmse:0.16397\n",
      "[1636]\ttrain-rmse:0.00187\teval-rmse:0.16397\n",
      "[1637]\ttrain-rmse:0.00186\teval-rmse:0.16397\n",
      "[1638]\ttrain-rmse:0.00186\teval-rmse:0.16397\n",
      "[1639]\ttrain-rmse:0.00186\teval-rmse:0.16397\n",
      "[1640]\ttrain-rmse:0.00185\teval-rmse:0.16398\n",
      "[1641]\ttrain-rmse:0.00185\teval-rmse:0.16398\n",
      "[1642]\ttrain-rmse:0.00184\teval-rmse:0.16398\n",
      "[1643]\ttrain-rmse:0.00184\teval-rmse:0.16398\n",
      "[1644]\ttrain-rmse:0.00183\teval-rmse:0.16398\n",
      "[1645]\ttrain-rmse:0.00183\teval-rmse:0.16398\n",
      "[1646]\ttrain-rmse:0.00182\teval-rmse:0.16398\n",
      "[1647]\ttrain-rmse:0.00182\teval-rmse:0.16398\n",
      "[1648]\ttrain-rmse:0.00181\teval-rmse:0.16398\n",
      "[1649]\ttrain-rmse:0.00181\teval-rmse:0.16398\n",
      "[1650]\ttrain-rmse:0.00181\teval-rmse:0.16398\n",
      "[1651]\ttrain-rmse:0.00180\teval-rmse:0.16398\n",
      "[1652]\ttrain-rmse:0.00180\teval-rmse:0.16398\n",
      "[1653]\ttrain-rmse:0.00179\teval-rmse:0.16398\n",
      "[1654]\ttrain-rmse:0.00179\teval-rmse:0.16398\n",
      "[1655]\ttrain-rmse:0.00178\teval-rmse:0.16398\n",
      "[1656]\ttrain-rmse:0.00178\teval-rmse:0.16398\n",
      "[1657]\ttrain-rmse:0.00178\teval-rmse:0.16398\n",
      "[1658]\ttrain-rmse:0.00177\teval-rmse:0.16398\n",
      "[1659]\ttrain-rmse:0.00177\teval-rmse:0.16399\n",
      "[1660]\ttrain-rmse:0.00176\teval-rmse:0.16399\n",
      "[1661]\ttrain-rmse:0.00176\teval-rmse:0.16399\n",
      "[1662]\ttrain-rmse:0.00175\teval-rmse:0.16399\n",
      "[1663]\ttrain-rmse:0.00175\teval-rmse:0.16399\n",
      "[1664]\ttrain-rmse:0.00175\teval-rmse:0.16399\n",
      "[1665]\ttrain-rmse:0.00174\teval-rmse:0.16399\n",
      "[1666]\ttrain-rmse:0.00174\teval-rmse:0.16399\n",
      "[1667]\ttrain-rmse:0.00173\teval-rmse:0.16399\n",
      "[1668]\ttrain-rmse:0.00173\teval-rmse:0.16399\n",
      "[1669]\ttrain-rmse:0.00172\teval-rmse:0.16399\n",
      "[1670]\ttrain-rmse:0.00172\teval-rmse:0.16399\n",
      "[1671]\ttrain-rmse:0.00172\teval-rmse:0.16399\n",
      "[1672]\ttrain-rmse:0.00171\teval-rmse:0.16399\n",
      "[1673]\ttrain-rmse:0.00171\teval-rmse:0.16399\n",
      "[1674]\ttrain-rmse:0.00170\teval-rmse:0.16399\n",
      "[1675]\ttrain-rmse:0.00170\teval-rmse:0.16399\n",
      "[1676]\ttrain-rmse:0.00170\teval-rmse:0.16400\n",
      "[1677]\ttrain-rmse:0.00169\teval-rmse:0.16400\n",
      "[1678]\ttrain-rmse:0.00169\teval-rmse:0.16400\n",
      "[1679]\ttrain-rmse:0.00168\teval-rmse:0.16400\n",
      "[1680]\ttrain-rmse:0.00168\teval-rmse:0.16400\n",
      "[1681]\ttrain-rmse:0.00168\teval-rmse:0.16400\n",
      "[1682]\ttrain-rmse:0.00167\teval-rmse:0.16400\n",
      "[1683]\ttrain-rmse:0.00167\teval-rmse:0.16400\n",
      "[1684]\ttrain-rmse:0.00166\teval-rmse:0.16400\n",
      "[1685]\ttrain-rmse:0.00166\teval-rmse:0.16400\n",
      "[1686]\ttrain-rmse:0.00166\teval-rmse:0.16400\n",
      "[1687]\ttrain-rmse:0.00165\teval-rmse:0.16400\n",
      "[1688]\ttrain-rmse:0.00165\teval-rmse:0.16400\n",
      "[1689]\ttrain-rmse:0.00164\teval-rmse:0.16400\n",
      "[1690]\ttrain-rmse:0.00164\teval-rmse:0.16400\n",
      "[1691]\ttrain-rmse:0.00163\teval-rmse:0.16400\n",
      "[1692]\ttrain-rmse:0.00163\teval-rmse:0.16400\n",
      "[1693]\ttrain-rmse:0.00163\teval-rmse:0.16400\n",
      "[1694]\ttrain-rmse:0.00162\teval-rmse:0.16400\n",
      "[1695]\ttrain-rmse:0.00162\teval-rmse:0.16400\n",
      "[1696]\ttrain-rmse:0.00162\teval-rmse:0.16400\n",
      "[1697]\ttrain-rmse:0.00161\teval-rmse:0.16400\n",
      "[1698]\ttrain-rmse:0.00161\teval-rmse:0.16400\n",
      "[1699]\ttrain-rmse:0.00160\teval-rmse:0.16400\n",
      "[1700]\ttrain-rmse:0.00160\teval-rmse:0.16400\n",
      "[1701]\ttrain-rmse:0.00160\teval-rmse:0.16400\n",
      "[1702]\ttrain-rmse:0.00159\teval-rmse:0.16400\n",
      "[1703]\ttrain-rmse:0.00159\teval-rmse:0.16400\n",
      "[1704]\ttrain-rmse:0.00159\teval-rmse:0.16400\n",
      "[1705]\ttrain-rmse:0.00158\teval-rmse:0.16400\n",
      "[1706]\ttrain-rmse:0.00158\teval-rmse:0.16400\n",
      "[1707]\ttrain-rmse:0.00157\teval-rmse:0.16400\n",
      "[1708]\ttrain-rmse:0.00157\teval-rmse:0.16400\n",
      "[1709]\ttrain-rmse:0.00157\teval-rmse:0.16400\n",
      "[1710]\ttrain-rmse:0.00156\teval-rmse:0.16400\n",
      "[1711]\ttrain-rmse:0.00156\teval-rmse:0.16400\n",
      "[1712]\ttrain-rmse:0.00155\teval-rmse:0.16400\n",
      "[1713]\ttrain-rmse:0.00155\teval-rmse:0.16400\n",
      "[1714]\ttrain-rmse:0.00155\teval-rmse:0.16400\n",
      "[1715]\ttrain-rmse:0.00154\teval-rmse:0.16400\n",
      "[1716]\ttrain-rmse:0.00154\teval-rmse:0.16400\n",
      "[1717]\ttrain-rmse:0.00154\teval-rmse:0.16400\n",
      "[1718]\ttrain-rmse:0.00153\teval-rmse:0.16400\n",
      "[1719]\ttrain-rmse:0.00153\teval-rmse:0.16400\n",
      "[1720]\ttrain-rmse:0.00153\teval-rmse:0.16400\n",
      "[1721]\ttrain-rmse:0.00152\teval-rmse:0.16400\n",
      "[1722]\ttrain-rmse:0.00152\teval-rmse:0.16400\n",
      "[1723]\ttrain-rmse:0.00151\teval-rmse:0.16400\n",
      "[1724]\ttrain-rmse:0.00151\teval-rmse:0.16401\n",
      "[1725]\ttrain-rmse:0.00151\teval-rmse:0.16401\n",
      "[1726]\ttrain-rmse:0.00150\teval-rmse:0.16401\n",
      "[1727]\ttrain-rmse:0.00150\teval-rmse:0.16401\n",
      "[1728]\ttrain-rmse:0.00150\teval-rmse:0.16401\n",
      "[1729]\ttrain-rmse:0.00149\teval-rmse:0.16401\n",
      "[1730]\ttrain-rmse:0.00149\teval-rmse:0.16401\n",
      "[1731]\ttrain-rmse:0.00148\teval-rmse:0.16401\n",
      "[1732]\ttrain-rmse:0.00148\teval-rmse:0.16401\n",
      "[1733]\ttrain-rmse:0.00148\teval-rmse:0.16401\n",
      "[1734]\ttrain-rmse:0.00147\teval-rmse:0.16401\n",
      "[1735]\ttrain-rmse:0.00147\teval-rmse:0.16401\n",
      "[1736]\ttrain-rmse:0.00147\teval-rmse:0.16401\n",
      "[1737]\ttrain-rmse:0.00146\teval-rmse:0.16401\n",
      "[1738]\ttrain-rmse:0.00146\teval-rmse:0.16401\n",
      "[1739]\ttrain-rmse:0.00146\teval-rmse:0.16401\n",
      "[1740]\ttrain-rmse:0.00145\teval-rmse:0.16401\n",
      "[1741]\ttrain-rmse:0.00145\teval-rmse:0.16401\n",
      "[1742]\ttrain-rmse:0.00145\teval-rmse:0.16401\n",
      "[1743]\ttrain-rmse:0.00144\teval-rmse:0.16401\n",
      "[1744]\ttrain-rmse:0.00144\teval-rmse:0.16401\n",
      "[1745]\ttrain-rmse:0.00144\teval-rmse:0.16401\n",
      "[1746]\ttrain-rmse:0.00143\teval-rmse:0.16401\n",
      "[1747]\ttrain-rmse:0.00143\teval-rmse:0.16401\n",
      "[1748]\ttrain-rmse:0.00143\teval-rmse:0.16401\n",
      "[1749]\ttrain-rmse:0.00142\teval-rmse:0.16401\n",
      "[1750]\ttrain-rmse:0.00142\teval-rmse:0.16401\n",
      "[1751]\ttrain-rmse:0.00142\teval-rmse:0.16401\n",
      "[1752]\ttrain-rmse:0.00141\teval-rmse:0.16401\n",
      "[1753]\ttrain-rmse:0.00141\teval-rmse:0.16401\n",
      "[1754]\ttrain-rmse:0.00141\teval-rmse:0.16401\n",
      "[1755]\ttrain-rmse:0.00140\teval-rmse:0.16401\n",
      "[1756]\ttrain-rmse:0.00140\teval-rmse:0.16401\n",
      "[1757]\ttrain-rmse:0.00140\teval-rmse:0.16401\n",
      "[1758]\ttrain-rmse:0.00139\teval-rmse:0.16401\n",
      "[1759]\ttrain-rmse:0.00139\teval-rmse:0.16401\n",
      "[1760]\ttrain-rmse:0.00139\teval-rmse:0.16401\n",
      "[1761]\ttrain-rmse:0.00138\teval-rmse:0.16401\n",
      "[1762]\ttrain-rmse:0.00138\teval-rmse:0.16401\n",
      "[1763]\ttrain-rmse:0.00138\teval-rmse:0.16402\n",
      "[1764]\ttrain-rmse:0.00137\teval-rmse:0.16402\n",
      "[1765]\ttrain-rmse:0.00137\teval-rmse:0.16402\n",
      "[1766]\ttrain-rmse:0.00137\teval-rmse:0.16402\n",
      "[1767]\ttrain-rmse:0.00136\teval-rmse:0.16402\n",
      "[1768]\ttrain-rmse:0.00136\teval-rmse:0.16402\n",
      "[1769]\ttrain-rmse:0.00136\teval-rmse:0.16402\n",
      "[1770]\ttrain-rmse:0.00136\teval-rmse:0.16402\n",
      "[1771]\ttrain-rmse:0.00135\teval-rmse:0.16402\n",
      "[1772]\ttrain-rmse:0.00135\teval-rmse:0.16402\n",
      "[1773]\ttrain-rmse:0.00135\teval-rmse:0.16402\n",
      "[1774]\ttrain-rmse:0.00134\teval-rmse:0.16402\n",
      "[1775]\ttrain-rmse:0.00134\teval-rmse:0.16402\n",
      "[1776]\ttrain-rmse:0.00134\teval-rmse:0.16402\n",
      "[1777]\ttrain-rmse:0.00133\teval-rmse:0.16402\n",
      "[1778]\ttrain-rmse:0.00133\teval-rmse:0.16402\n",
      "[1779]\ttrain-rmse:0.00133\teval-rmse:0.16402\n",
      "[1780]\ttrain-rmse:0.00132\teval-rmse:0.16402\n",
      "[1781]\ttrain-rmse:0.00132\teval-rmse:0.16402\n",
      "[1782]\ttrain-rmse:0.00132\teval-rmse:0.16402\n",
      "[1783]\ttrain-rmse:0.00131\teval-rmse:0.16402\n",
      "[1784]\ttrain-rmse:0.00131\teval-rmse:0.16402\n",
      "[1785]\ttrain-rmse:0.00131\teval-rmse:0.16402\n",
      "[1786]\ttrain-rmse:0.00130\teval-rmse:0.16402\n",
      "[1787]\ttrain-rmse:0.00130\teval-rmse:0.16402\n",
      "[1788]\ttrain-rmse:0.00130\teval-rmse:0.16402\n",
      "[1789]\ttrain-rmse:0.00129\teval-rmse:0.16402\n",
      "[1790]\ttrain-rmse:0.00129\teval-rmse:0.16402\n",
      "[1791]\ttrain-rmse:0.00129\teval-rmse:0.16402\n",
      "[1792]\ttrain-rmse:0.00129\teval-rmse:0.16402\n",
      "[1793]\ttrain-rmse:0.00128\teval-rmse:0.16402\n",
      "[1794]\ttrain-rmse:0.00128\teval-rmse:0.16402\n",
      "[1795]\ttrain-rmse:0.00128\teval-rmse:0.16402\n",
      "[1796]\ttrain-rmse:0.00127\teval-rmse:0.16402\n",
      "[1797]\ttrain-rmse:0.00127\teval-rmse:0.16402\n",
      "[1798]\ttrain-rmse:0.00127\teval-rmse:0.16402\n",
      "[1799]\ttrain-rmse:0.00127\teval-rmse:0.16403\n",
      "[1800]\ttrain-rmse:0.00126\teval-rmse:0.16403\n",
      "[1801]\ttrain-rmse:0.00126\teval-rmse:0.16403\n",
      "[1802]\ttrain-rmse:0.00126\teval-rmse:0.16403\n",
      "[1803]\ttrain-rmse:0.00125\teval-rmse:0.16403\n",
      "[1804]\ttrain-rmse:0.00125\teval-rmse:0.16403\n",
      "[1805]\ttrain-rmse:0.00125\teval-rmse:0.16403\n",
      "[1806]\ttrain-rmse:0.00125\teval-rmse:0.16403\n",
      "[1807]\ttrain-rmse:0.00124\teval-rmse:0.16403\n",
      "[1808]\ttrain-rmse:0.00124\teval-rmse:0.16403\n",
      "[1809]\ttrain-rmse:0.00124\teval-rmse:0.16403\n",
      "[1810]\ttrain-rmse:0.00123\teval-rmse:0.16403\n",
      "[1811]\ttrain-rmse:0.00123\teval-rmse:0.16403\n",
      "[1812]\ttrain-rmse:0.00123\teval-rmse:0.16403\n",
      "[1813]\ttrain-rmse:0.00123\teval-rmse:0.16403\n",
      "[1814]\ttrain-rmse:0.00122\teval-rmse:0.16403\n",
      "[1815]\ttrain-rmse:0.00122\teval-rmse:0.16403\n",
      "[1816]\ttrain-rmse:0.00122\teval-rmse:0.16403\n",
      "[1817]\ttrain-rmse:0.00121\teval-rmse:0.16403\n",
      "[1818]\ttrain-rmse:0.00121\teval-rmse:0.16403\n",
      "[1819]\ttrain-rmse:0.00121\teval-rmse:0.16403\n",
      "[1820]\ttrain-rmse:0.00121\teval-rmse:0.16403\n",
      "[1821]\ttrain-rmse:0.00120\teval-rmse:0.16403\n",
      "[1822]\ttrain-rmse:0.00120\teval-rmse:0.16403\n",
      "[1823]\ttrain-rmse:0.00120\teval-rmse:0.16403\n",
      "[1824]\ttrain-rmse:0.00120\teval-rmse:0.16403\n",
      "[1825]\ttrain-rmse:0.00119\teval-rmse:0.16403\n",
      "[1826]\ttrain-rmse:0.00119\teval-rmse:0.16403\n",
      "[1827]\ttrain-rmse:0.00119\teval-rmse:0.16403\n",
      "[1828]\ttrain-rmse:0.00118\teval-rmse:0.16403\n",
      "[1829]\ttrain-rmse:0.00118\teval-rmse:0.16403\n",
      "[1830]\ttrain-rmse:0.00118\teval-rmse:0.16403\n",
      "[1831]\ttrain-rmse:0.00118\teval-rmse:0.16403\n",
      "[1832]\ttrain-rmse:0.00117\teval-rmse:0.16403\n",
      "[1833]\ttrain-rmse:0.00117\teval-rmse:0.16403\n",
      "[1834]\ttrain-rmse:0.00117\teval-rmse:0.16403\n",
      "[1835]\ttrain-rmse:0.00117\teval-rmse:0.16403\n",
      "[1836]\ttrain-rmse:0.00116\teval-rmse:0.16403\n",
      "[1837]\ttrain-rmse:0.00116\teval-rmse:0.16403\n",
      "[1838]\ttrain-rmse:0.00116\teval-rmse:0.16403\n",
      "[1839]\ttrain-rmse:0.00115\teval-rmse:0.16403\n",
      "[1840]\ttrain-rmse:0.00115\teval-rmse:0.16403\n",
      "[1841]\ttrain-rmse:0.00115\teval-rmse:0.16403\n",
      "[1842]\ttrain-rmse:0.00115\teval-rmse:0.16403\n",
      "[1843]\ttrain-rmse:0.00114\teval-rmse:0.16403\n",
      "[1844]\ttrain-rmse:0.00114\teval-rmse:0.16403\n",
      "[1845]\ttrain-rmse:0.00114\teval-rmse:0.16403\n",
      "[1846]\ttrain-rmse:0.00113\teval-rmse:0.16403\n",
      "[1847]\ttrain-rmse:0.00113\teval-rmse:0.16403\n",
      "[1848]\ttrain-rmse:0.00113\teval-rmse:0.16403\n",
      "[1849]\ttrain-rmse:0.00113\teval-rmse:0.16403\n",
      "[1850]\ttrain-rmse:0.00112\teval-rmse:0.16403\n",
      "[1851]\ttrain-rmse:0.00112\teval-rmse:0.16403\n",
      "[1852]\ttrain-rmse:0.00112\teval-rmse:0.16403\n",
      "[1853]\ttrain-rmse:0.00112\teval-rmse:0.16403\n",
      "[1854]\ttrain-rmse:0.00111\teval-rmse:0.16403\n",
      "[1855]\ttrain-rmse:0.00111\teval-rmse:0.16403\n",
      "[1856]\ttrain-rmse:0.00111\teval-rmse:0.16403\n",
      "[1857]\ttrain-rmse:0.00111\teval-rmse:0.16403\n",
      "[1858]\ttrain-rmse:0.00110\teval-rmse:0.16403\n",
      "[1859]\ttrain-rmse:0.00110\teval-rmse:0.16403\n",
      "[1860]\ttrain-rmse:0.00110\teval-rmse:0.16403\n",
      "[1861]\ttrain-rmse:0.00110\teval-rmse:0.16403\n",
      "[1862]\ttrain-rmse:0.00109\teval-rmse:0.16403\n",
      "[1863]\ttrain-rmse:0.00109\teval-rmse:0.16403\n",
      "[1864]\ttrain-rmse:0.00109\teval-rmse:0.16403\n",
      "[1865]\ttrain-rmse:0.00109\teval-rmse:0.16403\n",
      "[1866]\ttrain-rmse:0.00109\teval-rmse:0.16403\n",
      "[1867]\ttrain-rmse:0.00108\teval-rmse:0.16403\n",
      "[1868]\ttrain-rmse:0.00108\teval-rmse:0.16403\n",
      "[1869]\ttrain-rmse:0.00108\teval-rmse:0.16403\n",
      "[1870]\ttrain-rmse:0.00108\teval-rmse:0.16403\n",
      "[1871]\ttrain-rmse:0.00107\teval-rmse:0.16403\n",
      "[1872]\ttrain-rmse:0.00107\teval-rmse:0.16403\n",
      "[1873]\ttrain-rmse:0.00107\teval-rmse:0.16403\n",
      "[1874]\ttrain-rmse:0.00107\teval-rmse:0.16403\n",
      "[1875]\ttrain-rmse:0.00106\teval-rmse:0.16403\n",
      "[1876]\ttrain-rmse:0.00106\teval-rmse:0.16403\n",
      "[1877]\ttrain-rmse:0.00106\teval-rmse:0.16403\n",
      "[1878]\ttrain-rmse:0.00106\teval-rmse:0.16403\n",
      "[1879]\ttrain-rmse:0.00105\teval-rmse:0.16403\n",
      "[1880]\ttrain-rmse:0.00105\teval-rmse:0.16403\n",
      "[1881]\ttrain-rmse:0.00105\teval-rmse:0.16403\n",
      "[1882]\ttrain-rmse:0.00105\teval-rmse:0.16403\n",
      "[1883]\ttrain-rmse:0.00104\teval-rmse:0.16403\n",
      "[1884]\ttrain-rmse:0.00104\teval-rmse:0.16403\n",
      "[1885]\ttrain-rmse:0.00104\teval-rmse:0.16403\n",
      "[1886]\ttrain-rmse:0.00104\teval-rmse:0.16403\n",
      "[1887]\ttrain-rmse:0.00104\teval-rmse:0.16403\n",
      "[1888]\ttrain-rmse:0.00103\teval-rmse:0.16403\n",
      "[1889]\ttrain-rmse:0.00103\teval-rmse:0.16403\n",
      "[1890]\ttrain-rmse:0.00103\teval-rmse:0.16403\n",
      "[1891]\ttrain-rmse:0.00103\teval-rmse:0.16403\n",
      "[1892]\ttrain-rmse:0.00103\teval-rmse:0.16403\n",
      "[1893]\ttrain-rmse:0.00103\teval-rmse:0.16403\n",
      "[1894]\ttrain-rmse:0.00102\teval-rmse:0.16403\n",
      "[1895]\ttrain-rmse:0.00102\teval-rmse:0.16403\n",
      "[1896]\ttrain-rmse:0.00102\teval-rmse:0.16403\n",
      "[1897]\ttrain-rmse:0.00102\teval-rmse:0.16403\n",
      "[1898]\ttrain-rmse:0.00102\teval-rmse:0.16404\n",
      "[1899]\ttrain-rmse:0.00101\teval-rmse:0.16404\n",
      "[1900]\ttrain-rmse:0.00101\teval-rmse:0.16404\n",
      "[1901]\ttrain-rmse:0.00101\teval-rmse:0.16404\n",
      "[1902]\ttrain-rmse:0.00101\teval-rmse:0.16404\n",
      "[1903]\ttrain-rmse:0.00101\teval-rmse:0.16404\n",
      "[1904]\ttrain-rmse:0.00101\teval-rmse:0.16404\n",
      "[1905]\ttrain-rmse:0.00100\teval-rmse:0.16404\n",
      "[1906]\ttrain-rmse:0.00100\teval-rmse:0.16404\n",
      "[1907]\ttrain-rmse:0.00100\teval-rmse:0.16404\n",
      "[1908]\ttrain-rmse:0.00100\teval-rmse:0.16404\n",
      "[1909]\ttrain-rmse:0.00100\teval-rmse:0.16404\n",
      "[1910]\ttrain-rmse:0.00099\teval-rmse:0.16404\n",
      "[1911]\ttrain-rmse:0.00099\teval-rmse:0.16404\n",
      "[1912]\ttrain-rmse:0.00099\teval-rmse:0.16404\n",
      "[1913]\ttrain-rmse:0.00099\teval-rmse:0.16404\n",
      "[1914]\ttrain-rmse:0.00099\teval-rmse:0.16404\n",
      "[1915]\ttrain-rmse:0.00098\teval-rmse:0.16404\n",
      "[1916]\ttrain-rmse:0.00098\teval-rmse:0.16404\n",
      "[1917]\ttrain-rmse:0.00098\teval-rmse:0.16404\n",
      "[1918]\ttrain-rmse:0.00098\teval-rmse:0.16404\n",
      "[1919]\ttrain-rmse:0.00098\teval-rmse:0.16404\n",
      "[1920]\ttrain-rmse:0.00098\teval-rmse:0.16404\n",
      "[1921]\ttrain-rmse:0.00097\teval-rmse:0.16404\n",
      "[1922]\ttrain-rmse:0.00097\teval-rmse:0.16404\n",
      "[1923]\ttrain-rmse:0.00097\teval-rmse:0.16404\n",
      "[1924]\ttrain-rmse:0.00097\teval-rmse:0.16404\n",
      "[1925]\ttrain-rmse:0.00097\teval-rmse:0.16404\n",
      "[1926]\ttrain-rmse:0.00097\teval-rmse:0.16404\n",
      "[1927]\ttrain-rmse:0.00096\teval-rmse:0.16404\n",
      "[1928]\ttrain-rmse:0.00096\teval-rmse:0.16404\n",
      "[1929]\ttrain-rmse:0.00096\teval-rmse:0.16404\n",
      "[1930]\ttrain-rmse:0.00096\teval-rmse:0.16404\n",
      "[1931]\ttrain-rmse:0.00096\teval-rmse:0.16404\n",
      "[1932]\ttrain-rmse:0.00096\teval-rmse:0.16404\n",
      "[1933]\ttrain-rmse:0.00095\teval-rmse:0.16404\n",
      "[1934]\ttrain-rmse:0.00095\teval-rmse:0.16404\n",
      "[1935]\ttrain-rmse:0.00095\teval-rmse:0.16404\n",
      "[1936]\ttrain-rmse:0.00095\teval-rmse:0.16404\n",
      "[1937]\ttrain-rmse:0.00095\teval-rmse:0.16404\n",
      "[1938]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1939]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1940]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1941]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1942]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1943]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1944]\ttrain-rmse:0.00094\teval-rmse:0.16404\n",
      "[1945]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1946]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1947]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1948]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1949]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1950]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1951]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1952]\ttrain-rmse:0.00093\teval-rmse:0.16404\n",
      "[1953]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1954]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1955]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1956]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1957]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1958]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1959]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1960]\ttrain-rmse:0.00092\teval-rmse:0.16404\n",
      "[1961]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1962]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1963]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1964]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1965]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1966]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1967]\ttrain-rmse:0.00091\teval-rmse:0.16404\n",
      "[1968]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1969]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1970]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1971]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1972]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1973]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1974]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1975]\ttrain-rmse:0.00090\teval-rmse:0.16404\n",
      "[1976]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1977]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1978]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1979]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1980]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1981]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1982]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1983]\ttrain-rmse:0.00089\teval-rmse:0.16404\n",
      "[1984]\ttrain-rmse:0.00089\teval-rmse:0.16405\n",
      "[1985]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1986]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1987]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1988]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1989]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1990]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1991]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1992]\ttrain-rmse:0.00088\teval-rmse:0.16405\n",
      "[1993]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "[1994]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "[1995]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "[1996]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "[1997]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "[1998]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "[1999]\ttrain-rmse:0.00087\teval-rmse:0.16405\n",
      "0.16404718319371456\n"
     ]
    }
   ],
   "source": [
    "num_round = 2000\n",
    "\n",
    "preds_XG = []\n",
    "rmses = []\n",
    "va_idxes_XG = []\n",
    "va_preds_XG = []\n",
    "\n",
    "for tr_idx,va_idx in KFold.split(train_x):\n",
    "    \n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(tr_x, label = tr_y)\n",
    "    dvalid  = xgb.DMatrix(va_x, label = va_y)\n",
    "\n",
    "    dtest  = xgb.DMatrix(test)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    \n",
    "    xgb_model = xgb.train(xgb_params,dtrain, num_round,evals = watchlist)\n",
    "\n",
    "    va_pred = xgb_model.predict(dvalid)\n",
    "\n",
    "    va_preds_XG.append(va_pred)\n",
    "    \n",
    "    va_idxes_XG.append(va_idx)\n",
    "\n",
    "    tmp_rmse = np.sqrt(mean_squared_error(va_pred, va_y))\n",
    "    rmses.append(tmp_rmse)\n",
    "    print(tmp_rmse)\n",
    "\n",
    "    pred = xgb_model.predict(dtest)\n",
    "    preds_XG.append(pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id    Pred_XG\n",
       "0   1  11.847257\n",
       "1   7  11.829121\n",
       "2  10  12.130093\n",
       "3  16  12.020136\n",
       "4  23  12.012076"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_XG</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11.847257</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>11.829121</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>12.130093</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>12.020136</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>12.012076</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "va_idxes_XG = np.concatenate(va_idxes_XG)\n",
    "va_pred_XG = np.concatenate(va_preds_XG,axis = 0)\n",
    "va_pred_XG = pd.DataFrame(data = va_pred_XG)\n",
    "order       = pd.DataFrame(data = va_idxes_XG)\n",
    "pred_train_XG = pd.concat([order,va_pred_XG],axis = 1)\n",
    "pred_train_XG.columns = [\"id\",\"Pred_XG\"]\n",
    "\n",
    "preds_test_XG = np.mean(preds_XG, axis = 0)\n",
    "\n",
    "pred_train_XG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "pred_train_lgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "pred_train_RF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "pred_train_LR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  Pred_LGBM\n",
       "0   1  11.920807\n",
       "1   7  11.825732\n",
       "2  10  12.134273\n",
       "3  16  11.992688\n",
       "4  23  12.134359"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_LGBM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11.920807</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>11.825732</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>12.134273</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>11.992688</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>12.134359</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "pred_train_lgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id  Pred_LGBM\n",
       "900    0  11.965755\n",
       "0      1  11.920807\n",
       "1200   2  11.818484\n",
       "2400   3  11.876481\n",
       "1500   4  12.091932"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_LGBM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>900</th>\n      <td>0</td>\n      <td>11.965755</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11.920807</td>\n    </tr>\n    <tr>\n      <th>1200</th>\n      <td>2</td>\n      <td>11.818484</td>\n    </tr>\n    <tr>\n      <th>2400</th>\n      <td>3</td>\n      <td>11.876481</td>\n    </tr>\n    <tr>\n      <th>1500</th>\n      <td>4</td>\n      <td>12.091932</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "pred_train_lgb = pred_train_lgb.sort_values('id')\n",
    "pred_train_lgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_RF = pred_train_RF.sort_values('id')\n",
    "pred_train_LR = pred_train_LR.sort_values('id')\n",
    "pred_train_TN = pred_train_TN.sort_values('id')\n",
    "pred_train_XG = pred_train_XG.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  Pred_LGBM    Pred_RF    Pred_LR    Pred_TN    Pred_XG\n",
       "0   0  11.965755  11.971192  11.982976  11.909129  11.970339\n",
       "1   1  11.920807  11.899078  11.990569  11.929918  11.847257\n",
       "2   2  11.818484  11.788391  12.014525  11.978004  11.794640\n",
       "3   3  11.876481  11.975346  11.826062  11.734773  11.877540\n",
       "4   4  12.091932  11.918731  11.912285  11.945896  12.054052"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Pred_LGBM</th>\n      <th>Pred_RF</th>\n      <th>Pred_LR</th>\n      <th>Pred_TN</th>\n      <th>Pred_XG</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>11.965755</td>\n      <td>11.971192</td>\n      <td>11.982976</td>\n      <td>11.909129</td>\n      <td>11.970339</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11.920807</td>\n      <td>11.899078</td>\n      <td>11.990569</td>\n      <td>11.929918</td>\n      <td>11.847257</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>11.818484</td>\n      <td>11.788391</td>\n      <td>12.014525</td>\n      <td>11.978004</td>\n      <td>11.794640</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>11.876481</td>\n      <td>11.975346</td>\n      <td>11.826062</td>\n      <td>11.734773</td>\n      <td>11.877540</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>12.091932</td>\n      <td>11.918731</td>\n      <td>11.912285</td>\n      <td>11.945896</td>\n      <td>12.054052</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "train_pred = pd.merge(pred_train_lgb, pred_train_RF, how = 'left', on = 'id')\n",
    "train_pred = pd.merge(train_pred,pred_train_LR, how = 'left', on = 'id')\n",
    "train_pred = pd.merge(train_pred,pred_train_TN, how = 'left', on = 'id')\n",
    "train_pred = pd.merge(train_pred,pred_train_XG, how = 'left', on = 'id')\n",
    "train_pred.head()\n",
    "#test_pred = pd.DataFrame({'pred_LGBM':pred_test_lgb,'pred_RF':pred_test_RF,'pred_LR':pred_test_LR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_pred['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\n2層目\\nridge回帰\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "2層目\n",
    "ridge回帰\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KFold = KFold(n_splits = 5, shuffle = True, random_state = 1223)\n",
    "#KFold = KFold(n_splits = 5, shuffle = True, random_state = 1223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Preds_lgb   Preds_RF   Preds_LR   Preds_TN   Preds_XG\n",
       "0  11.892250  11.914471  11.852434  11.995636  11.874422\n",
       "1  11.956680  11.971073  11.928764  11.903925  11.904577\n",
       "2  11.906136  11.926738  11.811534  11.918651  11.817995\n",
       "3  11.873929  11.865620  11.901240  11.905617  11.826089\n",
       "4  11.972198  11.970550  11.921383  11.924900  11.971805"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Preds_lgb</th>\n      <th>Preds_RF</th>\n      <th>Preds_LR</th>\n      <th>Preds_TN</th>\n      <th>Preds_XG</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11.892250</td>\n      <td>11.914471</td>\n      <td>11.852434</td>\n      <td>11.995636</td>\n      <td>11.874422</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11.956680</td>\n      <td>11.971073</td>\n      <td>11.928764</td>\n      <td>11.903925</td>\n      <td>11.904577</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11.906136</td>\n      <td>11.926738</td>\n      <td>11.811534</td>\n      <td>11.918651</td>\n      <td>11.817995</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.873929</td>\n      <td>11.865620</td>\n      <td>11.901240</td>\n      <td>11.905617</td>\n      <td>11.826089</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11.972198</td>\n      <td>11.970550</td>\n      <td>11.921383</td>\n      <td>11.924900</td>\n      <td>11.971805</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "preds_test_lgb = pd.DataFrame(data=preds_test_lgb, columns = ['Preds_lgb'])\n",
    "preds_test_RF  = pd.DataFrame(data=preds_test_RF, columns = ['Preds_RF'])\n",
    "preds_test_LR =  pd.DataFrame(data=preds_test_LR, columns = ['Preds_LR'])\n",
    "preds_test_TN =  pd.DataFrame(data=preds_test_TN, columns = ['Preds_TN'])\n",
    "preds_test_XG =  pd.DataFrame(data=preds_test_XG, columns = ['Preds_XG'])\n",
    "\n",
    "test_pred = pd.concat([preds_test_lgb,preds_test_RF],axis = 1)\n",
    "test_pred = pd.concat([test_pred,preds_test_LR], axis = 1)\n",
    "test_pred = pd.concat([test_pred,preds_test_TN], axis = 1)\n",
    "test_pred = pd.concat([test_pred,preds_test_XG], axis = 1)\n",
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KFold = KFold(n_splits = 5, shuffle = True, random_state = 1223)\n",
    "preds_2nd_LR = []\n",
    "rmses_2nd = []\n",
    "va_idxes_2nd_LR = []\n",
    "va_preds_2nd_LR = []\n",
    "\n",
    "ridge_2nd_model = linear_model.Ridge(alpha = 0.01)\n",
    "\n",
    "for tr_idx, va_idx in KFold.split(train_pred):\n",
    "    \n",
    "    tr_x, va_x = train_pred.iloc[tr_idx], train_pred.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    ridge_2nd_model.fit(tr_x,tr_y)\n",
    "    \n",
    "    va_pred = ridge_2nd_model.predict(va_x)\n",
    "    \n",
    "    tmp_rmse = np.sqrt(mean_squared_error(np.exp(va_pred), np.exp(va_y)))\n",
    "    rmses_2nd.append(tmp_rmse)\n",
    "    \n",
    "    pred = ridge_2nd_model.predict(test_pred)\n",
    "    preds_2nd_LR.append(np.exp(pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26999.244527934312\n"
     ]
    }
   ],
   "source": [
    "rmse_mean =  sum(rmses_2nd)/len(rmses_2nd)\n",
    "print(rmse_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2nd_mean = np.mean(preds_2nd_LR, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               0\n",
       "0  147965.817494\n",
       "1  155305.110353\n",
       "2  148005.446212\n",
       "3  143005.013618\n",
       "4  158071.132172"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>147965.817494</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>155305.110353</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>148005.446212</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>143005.013618</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>158071.132172</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "pred_sub = pd.DataFrame(data = preds_2nd_mean)\n",
    "pred_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = pd.DataFrame(data = test['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index              0\n",
       "0    398  147965.817494\n",
       "1   3833  155305.110353\n",
       "2   4836  148005.446212\n",
       "3   4572  143005.013618\n",
       "4    636  158071.132172"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>398</td>\n      <td>147965.817494</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3833</td>\n      <td>155305.110353</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4836</td>\n      <td>148005.446212</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4572</td>\n      <td>143005.013618</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>636</td>\n      <td>158071.132172</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "sub_data = pd.concat([test_index,pred_sub],axis = 1)\n",
    "sub_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.to_csv('submission.csv',header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}